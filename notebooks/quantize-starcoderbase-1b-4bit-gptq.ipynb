{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:42:06.027128Z","iopub.execute_input":"2024-03-14T11:42:06.027444Z","iopub.status.idle":"2024-03-14T11:42:22.041078Z","shell.execute_reply.started":"2024-03-14T11:42:06.027418Z","shell.execute_reply":"2024-03-14T11:42:22.039759Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import random\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer\n\n\n# Define base model and output directory\nmodel_id = \"bigcode/starcoderbase-1b\"\nout_dir = model_id + \"-GPTQ\"","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:43:05.873190Z","iopub.execute_input":"2024-03-14T11:43:05.873561Z","iopub.status.idle":"2024-03-14T11:43:14.770907Z","shell.execute_reply.started":"2024-03-14T11:43:05.873528Z","shell.execute_reply":"2024-03-14T11:43:14.769919Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:43:19.985731Z","iopub.execute_input":"2024-03-14T11:43:19.986595Z","iopub.status.idle":"2024-03-14T11:43:20.035600Z","shell.execute_reply.started":"2024-03-14T11:43:19.986560Z","shell.execute_reply":"2024-03-14T11:43:20.034661Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:43:22.580512Z","iopub.execute_input":"2024-03-14T11:43:22.580913Z","iopub.status.idle":"2024-03-14T11:43:23.540425Z","shell.execute_reply.started":"2024-03-14T11:43:22.580883Z","shell.execute_reply":"2024-03-14T11:43:23.539247Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token <token> --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:44:16.991816Z","iopub.execute_input":"2024-03-14T11:44:16.992227Z","iopub.status.idle":"2024-03-14T11:44:18.668235Z","shell.execute_reply.started":"2024-03-14T11:44:16.992191Z","shell.execute_reply":"2024-03-14T11:44:18.667280Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:44:32.382088Z","iopub.execute_input":"2024-03-14T11:44:32.382471Z","iopub.status.idle":"2024-03-14T11:46:06.303330Z","shell.execute_reply.started":"2024-03-14T11:44:32.382438Z","shell.execute_reply":"2024-03-14T11:46:06.302528Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16256ae7ca3247f887255ee62b7ca8d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d78c662d4e04382bffa382ce2449239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d7a81621124951b24ff12b7a8babbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c7b81336244762b550c51e97333da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc9aa4f1d9854cd0a81818867ce6e25b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d04e9f27b2714b1691434e0547e12110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1172f5ae30004ea7924c2c859a9b69cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/532 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11206a265334871b4b124997f9f5ca7"}},"metadata":{}}]},{"cell_type":"code","source":"examples = [\n    tokenizer(\n        \"def add(x, y): \\n z = x+y \\n return z\"\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:47:04.358589Z","iopub.execute_input":"2024-03-14T11:47:04.359300Z","iopub.status.idle":"2024-03-14T11:47:04.367071Z","shell.execute_reply.started":"2024-03-14T11:47:04.359270Z","shell.execute_reply":"2024-03-14T11:47:04.366131Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Quantize with GPTQ\nmodel.quantize(\n    examples,\n    batch_size=1,\n    use_triton=True,\n)\n\n# Save model and tokenizer\nmodel.save_quantized(out_dir, use_safetensors=True)\ntokenizer.save_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:47:10.201354Z","iopub.execute_input":"2024-03-14T11:47:10.201731Z","iopub.status.idle":"2024-03-14T11:50:31.583974Z","shell.execute_reply.started":"2024-03-14T11:47:10.201700Z","shell.execute_reply":"2024-03-14T11:50:31.583083Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"WARNING - triton is not installed, reset use_triton to False\nINFO - Start quantizing layer 1/24\nINFO - Quantizing attn.c_attn in layer 1/24...\nINFO - Quantizing attn.c_proj in layer 1/24...\nINFO - Quantizing mlp.c_fc in layer 1/24...\nINFO - Quantizing mlp.c_proj in layer 1/24...\nINFO - Start quantizing layer 2/24\nINFO - Quantizing attn.c_attn in layer 2/24...\nINFO - Quantizing attn.c_proj in layer 2/24...\nINFO - Quantizing mlp.c_fc in layer 2/24...\nINFO - Quantizing mlp.c_proj in layer 2/24...\nINFO - Start quantizing layer 3/24\nINFO - Quantizing attn.c_attn in layer 3/24...\nINFO - Quantizing attn.c_proj in layer 3/24...\nINFO - Quantizing mlp.c_fc in layer 3/24...\nINFO - Quantizing mlp.c_proj in layer 3/24...\nINFO - Start quantizing layer 4/24\nINFO - Quantizing attn.c_attn in layer 4/24...\nINFO - Quantizing attn.c_proj in layer 4/24...\nINFO - Quantizing mlp.c_fc in layer 4/24...\nINFO - Quantizing mlp.c_proj in layer 4/24...\nINFO - Start quantizing layer 5/24\nINFO - Quantizing attn.c_attn in layer 5/24...\nINFO - Quantizing attn.c_proj in layer 5/24...\nINFO - Quantizing mlp.c_fc in layer 5/24...\nINFO - Quantizing mlp.c_proj in layer 5/24...\nINFO - Start quantizing layer 6/24\nINFO - Quantizing attn.c_attn in layer 6/24...\nINFO - Quantizing attn.c_proj in layer 6/24...\nINFO - Quantizing mlp.c_fc in layer 6/24...\nINFO - Quantizing mlp.c_proj in layer 6/24...\nINFO - Start quantizing layer 7/24\nINFO - Quantizing attn.c_attn in layer 7/24...\nINFO - Quantizing attn.c_proj in layer 7/24...\nINFO - Quantizing mlp.c_fc in layer 7/24...\nINFO - Quantizing mlp.c_proj in layer 7/24...\nINFO - Start quantizing layer 8/24\nINFO - Quantizing attn.c_attn in layer 8/24...\nINFO - Quantizing attn.c_proj in layer 8/24...\nINFO - Quantizing mlp.c_fc in layer 8/24...\nINFO - Quantizing mlp.c_proj in layer 8/24...\nINFO - Start quantizing layer 9/24\nINFO - Quantizing attn.c_attn in layer 9/24...\nINFO - Quantizing attn.c_proj in layer 9/24...\nINFO - Quantizing mlp.c_fc in layer 9/24...\nINFO - Quantizing mlp.c_proj in layer 9/24...\nINFO - Start quantizing layer 10/24\nINFO - Quantizing attn.c_attn in layer 10/24...\nINFO - Quantizing attn.c_proj in layer 10/24...\nINFO - Quantizing mlp.c_fc in layer 10/24...\nINFO - Quantizing mlp.c_proj in layer 10/24...\nINFO - Start quantizing layer 11/24\nINFO - Quantizing attn.c_attn in layer 11/24...\nINFO - Quantizing attn.c_proj in layer 11/24...\nINFO - Quantizing mlp.c_fc in layer 11/24...\nINFO - Quantizing mlp.c_proj in layer 11/24...\nINFO - Start quantizing layer 12/24\nINFO - Quantizing attn.c_attn in layer 12/24...\nINFO - Quantizing attn.c_proj in layer 12/24...\nINFO - Quantizing mlp.c_fc in layer 12/24...\nINFO - Quantizing mlp.c_proj in layer 12/24...\nINFO - Start quantizing layer 13/24\nINFO - Quantizing attn.c_attn in layer 13/24...\nINFO - Quantizing attn.c_proj in layer 13/24...\nINFO - Quantizing mlp.c_fc in layer 13/24...\nINFO - Quantizing mlp.c_proj in layer 13/24...\nINFO - Start quantizing layer 14/24\nINFO - Quantizing attn.c_attn in layer 14/24...\nINFO - Quantizing attn.c_proj in layer 14/24...\nINFO - Quantizing mlp.c_fc in layer 14/24...\nINFO - Quantizing mlp.c_proj in layer 14/24...\nINFO - Start quantizing layer 15/24\nINFO - Quantizing attn.c_attn in layer 15/24...\nINFO - Quantizing attn.c_proj in layer 15/24...\nINFO - Quantizing mlp.c_fc in layer 15/24...\nINFO - Quantizing mlp.c_proj in layer 15/24...\nINFO - Start quantizing layer 16/24\nINFO - Quantizing attn.c_attn in layer 16/24...\nINFO - Quantizing attn.c_proj in layer 16/24...\nINFO - Quantizing mlp.c_fc in layer 16/24...\nINFO - Quantizing mlp.c_proj in layer 16/24...\nINFO - Start quantizing layer 17/24\nINFO - Quantizing attn.c_attn in layer 17/24...\nINFO - Quantizing attn.c_proj in layer 17/24...\nINFO - Quantizing mlp.c_fc in layer 17/24...\nINFO - Quantizing mlp.c_proj in layer 17/24...\nINFO - Start quantizing layer 18/24\nINFO - Quantizing attn.c_attn in layer 18/24...\nINFO - Quantizing attn.c_proj in layer 18/24...\nINFO - Quantizing mlp.c_fc in layer 18/24...\nINFO - Quantizing mlp.c_proj in layer 18/24...\nINFO - Start quantizing layer 19/24\nINFO - Quantizing attn.c_attn in layer 19/24...\nINFO - Quantizing attn.c_proj in layer 19/24...\nINFO - Quantizing mlp.c_fc in layer 19/24...\nINFO - Quantizing mlp.c_proj in layer 19/24...\nINFO - Start quantizing layer 20/24\nINFO - Quantizing attn.c_attn in layer 20/24...\nINFO - Quantizing attn.c_proj in layer 20/24...\nINFO - Quantizing mlp.c_fc in layer 20/24...\nINFO - Quantizing mlp.c_proj in layer 20/24...\nINFO - Start quantizing layer 21/24\nINFO - Quantizing attn.c_attn in layer 21/24...\nINFO - Quantizing attn.c_proj in layer 21/24...\nINFO - Quantizing mlp.c_fc in layer 21/24...\nINFO - Quantizing mlp.c_proj in layer 21/24...\nINFO - Start quantizing layer 22/24\nINFO - Quantizing attn.c_attn in layer 22/24...\nINFO - Quantizing attn.c_proj in layer 22/24...\nINFO - Quantizing mlp.c_fc in layer 22/24...\nINFO - Quantizing mlp.c_proj in layer 22/24...\nINFO - Start quantizing layer 23/24\nINFO - Quantizing attn.c_attn in layer 23/24...\nINFO - Quantizing attn.c_proj in layer 23/24...\nINFO - Quantizing mlp.c_fc in layer 23/24...\nINFO - Quantizing mlp.c_proj in layer 23/24...\nINFO - Start quantizing layer 24/24\nINFO - Quantizing attn.c_attn in layer 24/24...\nINFO - Quantizing attn.c_proj in layer 24/24...\nINFO - Quantizing mlp.c_fc in layer 24/24...\nINFO - Quantizing mlp.c_proj in layer 24/24...\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 3min 26s, sys: 10.3 s, total: 3min 36s\nWall time: 3min 21s\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('bigcode/starcoderbase-1b-GPTQ/tokenizer_config.json',\n 'bigcode/starcoderbase-1b-GPTQ/special_tokens_map.json',\n 'bigcode/starcoderbase-1b-GPTQ/vocab.json',\n 'bigcode/starcoderbase-1b-GPTQ/merges.txt',\n 'bigcode/starcoderbase-1b-GPTQ/added_tokens.json',\n 'bigcode/starcoderbase-1b-GPTQ/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Reload model and tokenizer\nmodel = AutoGPTQForCausalLM.from_quantized(\n    out_dir,\n    device=device,\n    use_triton=True,\n    use_safetensors=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:50:52.501995Z","iopub.execute_input":"2024-03-14T11:50:52.502356Z","iopub.status.idle":"2024-03-14T11:50:53.662591Z","shell.execute_reply.started":"2024-03-14T11:50:52.502327Z","shell.execute_reply":"2024-03-14T11:50:53.661813Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"WARNING - Triton is not installed, reset use_triton to False.\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n","output_type":"stream"}]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:52:36.193747Z","iopub.execute_input":"2024-03-14T11:52:36.194486Z","iopub.status.idle":"2024-03-14T11:52:36.200212Z","shell.execute_reply.started":"2024-03-14T11:52:36.194456Z","shell.execute_reply":"2024-03-14T11:52:36.199347Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'UTF-8'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline, TextGenerationPipeline\n\n# or you can also use pipeline\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"def add(x, y)\")[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:52:39.159883Z","iopub.execute_input":"2024-03-14T11:52:39.160237Z","iopub.status.idle":"2024-03-14T11:52:55.243784Z","shell.execute_reply.started":"2024-03-14T11:52:39.160208Z","shell.execute_reply":"2024-03-14T11:52:55.242788Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-03-14 11:52:41.340902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 11:52:41.340995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 11:52:41.516145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nThe model 'GPTBigCodeGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"def add(x, y) {\n    return x + y;\n}\n\nfunction subtract\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q huggingface_hub\nfrom huggingface_hub import create_repo, HfApi\n# from google.colab import userdata\n\nusername = \"cosmo3769\"\nMODEL_NAME = \"starcoderbase-1b\"\nMAIN_PATH = \"bigcode\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=\"token\")\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GPTQ\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=f\"{MAIN_PATH}/{MODEL_NAME}-GPTQ\",\n    repo_id=f\"{username}/{MODEL_NAME}-GPTQ\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:53:27.855137Z","iopub.execute_input":"2024-03-14T11:53:27.856289Z","iopub.status.idle":"2024-03-14T11:54:08.572339Z","shell.execute_reply.started":"2024-03-14T11:53:27.856258Z","shell.execute_reply":"2024-03-14T11:54:08.571270Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gptq_model-4bit-128g.safetensors:   0%|          | 0.00/968M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78715d2891e4a65bcb15dd52c6e22fc"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/cosmo3769/starcoderbase-1b-GPTQ/commit/99e9d9cfbd22ae76509b29fbdc24ccd77e0682f2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='99e9d9cfbd22ae76509b29fbdc24ccd77e0682f2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
