{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Run evaluations on starcoderbase-1b non-quantized model\n\n[HuggingFace Model card for starcoderbase-1b](https://huggingface.co/bigcode/starcoderbase-1b)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/EleutherAI/lm-evaluation-harness","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:23:44.130959Z","iopub.execute_input":"2024-03-14T13:23:44.131773Z","iopub.status.idle":"2024-03-14T13:23:48.950826Z","shell.execute_reply.started":"2024-03-14T13:23:44.131741Z","shell.execute_reply":"2024-03-14T13:23:48.949349Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 32353, done.\u001b[K\nremote: Counting objects: 100% (915/915), done.\u001b[K\nremote: Compressing objects: 100% (560/560), done.\u001b[K\nremote: Total 32353 (delta 546), reused 635 (delta 351), pack-reused 31438\u001b[K\nReceiving objects: 100% (32353/32353), 22.82 MiB | 9.19 MiB/s, done.\nResolving deltas: 100% (22513/22513), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd lm-evaluation-harness","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:26:27.248166Z","iopub.execute_input":"2024-03-14T13:26:27.248629Z","iopub.status.idle":"2024-03-14T13:26:27.256540Z","shell.execute_reply.started":"2024-03-14T13:26:27.248590Z","shell.execute_reply":"2024-03-14T13:26:27.255425Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/lm-evaluation-harness\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:26:56.390183Z","iopub.execute_input":"2024-03-14T13:26:56.390559Z","iopub.status.idle":"2024-03-14T13:27:53.192996Z","shell.execute_reply.started":"2024-03-14T13:26:56.390532Z","shell.execute_reply":"2024-03-14T13:27:53.192037Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.27.2)\nCollecting evaluate (from lm_eval==0.4.1)\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting datasets>=2.16.0 (from lm_eval==0.4.1)\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\nCollecting jsonlines (from lm_eval==0.4.1)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.9.0)\nCollecting peft>=0.2.0 (from lm_eval==0.4.1)\n  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pybind11>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.11.1)\nCollecting pytablewriter (from lm_eval==0.4.1)\n  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval==0.4.1)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting sacrebleu>=1.5.0 (from lm_eval==0.4.1)\n  Downloading sacrebleu-2.4.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (1.2.2)\nCollecting sqlitedict (from lm_eval==0.4.1)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.1.2)\nCollecting tqdm-multiprocess (from lm_eval==0.4.1)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (4.38.1)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.22.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.3.8)\nCollecting word2number (from lm_eval==0.4.1)\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (10.2.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.13.1)\nCollecting pyarrow>=12.0.0 (from datasets>=2.16.0->lm_eval==0.4.1)\n  Downloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pyarrow-hotfix (from datasets>=2.16.0->lm_eval==0.4.1)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->lm_eval==0.4.1) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.16.0)\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.1)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (5.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.1.2)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==0.4.1) (0.15.2)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->lm_eval==0.4.1) (23.2.0)\nRequirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (69.0.3)\nCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.4.1)\n  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.1)\n  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.1)\n  Downloading pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.1)\n  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.1)\n  Downloading tcolorpy-0.1.4-py3-none-any.whl.metadata (5.7 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1)\n  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (4.0.3)\nCollecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.1)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.21.0->lm_eval==0.4.1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2.8.2)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2023.3.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==0.4.1) (2.1.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.1) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==0.4.1) (1.3.0)\nDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.9.0-py3-none-any.whl (190 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\nDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\nDownloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\nDownloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\nDownloading typepy-1.3.2-py3-none-any.whl (31 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.1-0.editable-py3-none-any.whl size=15041 sha256=e57093b337983af4d05738e07644ae82878a1eb228d804100e756f1f02b9af53\n  Stored in directory: /tmp/pip-ephem-wheel-cache-c46hkegq/wheels/1b/1a/1b/44c80ddb18c9d7d3ce79a8d6d4561bddaddcbffb4cdfbf3259\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c77551e0107d7a31ba9b8041b54eba17175c64ee64480e048adb1bf6f63854ea\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=eb1e0ead5f7687d9dc93dbaf69d1a51d201a122c31fcac3dd645f41b36623f65\n  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=c3a6779726c78c04dfc8f891c3c9b4e9b57ce2e78ffebaf6c089a5cdd58b4a8f\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built lm_eval rouge-score sqlitedict word2number\nInstalling collected packages: word2number, sqlitedict, tqdm-multiprocess, tcolorpy, pyarrow-hotfix, pyarrow, portalocker, pathvalidate, jsonlines, chardet, sacrebleu, rouge-score, mbstrdecoder, typepy, datasets, peft, evaluate, DataProperty, tabledata, pytablewriter, lm_eval\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed DataProperty-1.0.1 chardet-5.2.0 datasets-2.18.0 evaluate-0.4.1 jsonlines-4.0.0 lm_eval-0.4.1 mbstrdecoder-1.1.3 pathvalidate-3.2.0 peft-0.9.0 portalocker-2.8.2 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.1 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 word2number-1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade --q wandb\n!wandb login 26dc38c2c67a238da52038bccb87546d40f8d428","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:33:27.266278Z","iopub.execute_input":"2024-03-14T13:33:27.266628Z","iopub.status.idle":"2024-03-14T13:33:42.994319Z","shell.execute_reply.started":"2024-03-14T13:33:27.266596Z","shell.execute_reply":"2024-03-14T13:33:42.993162Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"!git config --global credential.helper store\n!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:33:52.596507Z","iopub.execute_input":"2024-03-14T13:33:52.596930Z","iopub.status.idle":"2024-03-14T13:33:55.220683Z","shell.execute_reply.started":"2024-03-14T13:33:52.596874Z","shell.execute_reply":"2024-03-14T13:33:55.219713Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### bigbench_code_line_description_generate_until","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks bigbench_code_line_description_generate_until \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:33:57.692296Z","iopub.execute_input":"2024-03-14T13:33:57.692949Z","iopub.status.idle":"2024-03-14T13:36:16.143279Z","shell.execute_reply.started":"2024-03-14T13:33:57.692906Z","shell.execute_reply":"2024-03-14T13:36:16.142061Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2024-03-14 13:34:02.454153: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:34:02.454211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:34:02.455627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_133409-wgzliubi\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweet-potato-pastry-75\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/wgzliubi\u001b[0m\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.05k/1.05k [00:00<00:00, 2.78MB/s]\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.55G/4.55G [00:13<00:00, 346MB/s]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:00<00:00, 240kB/s]\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 677/677 [00:00<00:00, 1.97MB/s]\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 777k/777k [00:00<00:00, 2.05MB/s]\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 442k/442k [00:00<00:00, 1.80MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.06M/2.06M [00:01<00:00, 1.30MB/s]\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 532/532 [00:00<00:00, 1.44MB/s]\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130k/130k [00:00<00:00, 1.48MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.8k/17.8k [00:00<00:00, 28.8kB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.3k/15.3k [00:00<00:00, 27.0kB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.25k/9.25k [00:00<00:00, 17.4kB/s]\nGenerating default split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 7838.60 examples/s]\nGenerating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [00:00<00:00, 10889.80 examples/s]\nGenerating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 5587.75 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 32301.15it/s]\nRunning generate_until requests:   0%|                   | 0/60 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:12<00:00,  4.75it/s]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|                    Tasks                    |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n|---------------------------------------------|------:|------|------|-----------|----:|---|-----:|\n|bigbench_code_line_description_generate_until|      1|none  |None  |exact_match|    0|¬±  |     0|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:              bigbench_code_line_description_generate_until/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msweet-potato-pastry-75\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/wgzliubi\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_133409-wgzliubi/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### bigbench_code_line_description_multiple_choice","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks bigbench_code_line_description_multiple_choice \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:36:29.984451Z","iopub.execute_input":"2024-03-14T13:36:29.984865Z","iopub.status.idle":"2024-03-14T13:39:03.277474Z","shell.execute_reply.started":"2024-03-14T13:36:29.984829Z","shell.execute_reply":"2024-03-14T13:39:03.276246Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2024-03-14 13:36:34.725287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:36:34.725349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:36:34.726798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_133641-ff3w2b79\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbutterscotch-flambee-76\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/ff3w2b79\u001b[0m\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 1995.32it/s]\nRunning loglikelihood requests:   0%|                   | 0/242 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\nDetermined largest batch size: 64\nRunning loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 242/242 [00:48<00:00,  5.02it/s]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n|                    Tasks                     |Version|Filter|n-shot|Metric|Value|   |Stderr|\n|----------------------------------------------|------:|------|------|------|----:|---|-----:|\n|bigbench_code_line_description_multiple_choice|      0|none  |None  |acc   | 0.15|¬±  |0.0465|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc 0.15\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr 0.04649\n\u001b[34m\u001b[1mwandb\u001b[0m:      bigbench_code_line_description_multiple_choice/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbutterscotch-flambee-76\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/ff3w2b79\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_133641-ff3w2b79/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_go","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_go \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --limit 10 \\\n    --output_path output/starcoderbase-1b \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:43:45.021224Z","iopub.execute_input":"2024-03-14T13:43:45.021979Z","iopub.status.idle":"2024-03-14T13:47:31.458770Z","shell.execute_reply.started":"2024-03-14T13:43:45.021932Z","shell.execute_reply":"2024-03-14T13:47:31.457735Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"2024-03-14 13:43:49.921684: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:43:49.921759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:43:49.923337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_134356-5jdfojlo\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcinnamon-flambee-78\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/5jdfojlo\u001b[0m\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14237.28it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.69s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|   Tasks    |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_go|      1|none  |None  |smoothed_bleu_4|1.1793|¬±  |0.0411|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_go/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_go/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_go/alias code2text_go\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_go/smoothed_bleu_4 1.17929\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_go/smoothed_bleu_4_stderr 0.04112\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcinnamon-flambee-78\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/5jdfojlo\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_134356-5jdfojlo/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_java","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_java \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:47:45.107812Z","iopub.execute_input":"2024-03-14T13:47:45.108663Z","iopub.status.idle":"2024-03-14T13:52:07.558939Z","shell.execute_reply.started":"2024-03-14T13:47:45.108624Z","shell.execute_reply":"2024-03-14T13:52:07.557911Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2024-03-14 13:47:50.129313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:47:50.129364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:47:50.130912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_134757-rcjmhguf\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mberry-strudel-79\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/rcjmhguf\u001b[0m\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 913/913 [00:00<00:00, 2.65MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141M/141M [00:27<00:00, 5.18MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.25M/4.25M [00:01<00:00, 3.22MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.38M/9.38M [00:01<00:00, 5.80MB/s]\nGenerating train split: 100%|‚ñà| 164923/164923 [00:01<00:00, 119981.09 examples/s\nGenerating validation split: 100%|‚ñà| 5183/5183 [00:00<00:00, 112732.33 examples/\nGenerating test split: 100%|‚ñà‚ñà‚ñà| 10955/10955 [00:00<00:00, 116694.27 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14593.96it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:12<00:00,  7.26s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|    Tasks     |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|--------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_java|      1|none  |None  |smoothed_bleu_4|0.844|¬±  |0.1861|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_java/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_java/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_java/alias code2text_java\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_java/smoothed_bleu_4 0.84399\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_java/smoothed_bleu_4_stderr 0.18605\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mberry-strudel-79\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/rcjmhguf\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_134757-rcjmhguf/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_javascript","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_javascript \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:53:02.074038Z","iopub.execute_input":"2024-03-14T13:53:02.074846Z","iopub.status.idle":"2024-03-14T13:56:33.932721Z","shell.execute_reply.started":"2024-03-14T13:53:02.074810Z","shell.execute_reply":"2024-03-14T13:56:33.931665Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"2024-03-14 13:53:06.924093: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:53:06.924152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:53:06.925683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_135314-d55ctwzl\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhershey-crumble-80\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/d55ctwzl\u001b[0m\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 916/916 [00:00<00:00, 2.85MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58.4M/58.4M [00:12<00:00, 4.55MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.78M/3.78M [00:01<00:00, 2.87MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.59M/3.59M [00:01<00:00, 2.77MB/s]\nGenerating train split: 100%|‚ñà‚ñà‚ñà| 58025/58025 [00:00<00:00, 92334.17 examples/s]\nGenerating validation split: 100%|‚ñà| 3885/3885 [00:00<00:00, 87807.00 examples/s\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3291/3291 [00:00<00:00, 77134.95 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 12449.70it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:13<00:00,  7.33s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|       Tasks        |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|--------------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_javascript|      1|none  |None  |smoothed_bleu_4|1.0811|¬±  |0.1305|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias code2text_javascript...\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 1.08112\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 0.13046\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mhershey-crumble-80\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/d55ctwzl\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_135314-d55ctwzl/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_php","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_php \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 8 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:56:46.309380Z","iopub.execute_input":"2024-03-14T13:56:46.310163Z","iopub.status.idle":"2024-03-14T14:01:45.589857Z","shell.execute_reply.started":"2024-03-14T13:56:46.310121Z","shell.execute_reply":"2024-03-14T14:01:45.588800Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"2024-03-14 13:56:51.187097: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 13:56:51.187150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 13:56:51.188725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_135658-0ki3lv0n\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwalnut-pastry-81\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/0ki3lv0n\u001b[0m\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 913/913 [00:00<00:00, 2.71MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98.2M/98.2M [00:23<00:00, 4.21MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99.8M/99.8M [00:16<00:00, 5.99MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.5M/10.5M [00:02<00:00, 3.85MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.2M/11.2M [00:02<00:00, 4.26MB/s]\nGenerating train split: 100%|‚ñà| 241241/241241 [00:02<00:00, 117379.56 examples/s\nGenerating validation split: 100%|‚ñà| 12982/12982 [00:00<00:00, 115027.26 example\nGenerating test split: 100%|‚ñà‚ñà‚ñà| 14014/14014 [00:00<00:00, 119273.83 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12510.97it/s]\nRunning generate_until requests:   0%|                    | 0/8 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:06<00:00,  8.29s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 8.0, num_fewshot: None, batch_size: auto\n|    Tasks    |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|-------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_php|      1|none  |None  |smoothed_bleu_4| 1.06|¬±  |0.1123|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_php/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_php/alias code2text_php\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4 1.05998\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_php/smoothed_bleu_4_stderr 0.11234\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mwalnut-pastry-81\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/0ki3lv0n\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_135658-0ki3lv0n/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_python","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_python \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 2 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:01:56.075308Z","iopub.execute_input":"2024-03-14T14:01:56.076230Z","iopub.status.idle":"2024-03-14T14:06:53.896606Z","shell.execute_reply.started":"2024-03-14T14:01:56.076189Z","shell.execute_reply":"2024-03-14T14:06:53.895541Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"2024-03-14 14:02:00.956272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:02:00.956332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:02:00.957799: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_140209-twm0h6ct\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-pie-82\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/twm0h6ct\u001b[0m\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 916/916 [00:00<00:00, 2.99MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147M/147M [00:30<00:00, 4.79MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144M/144M [00:36<00:00, 3.94MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.7M/16.7M [00:06<00:00, 2.53MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.0M/18.0M [00:03<00:00, 4.81MB/s]\nGenerating train split: 100%|‚ñà| 251820/251820 [00:02<00:00, 101431.03 examples/s\nGenerating validation split: 100%|‚ñà| 13914/13914 [00:00<00:00, 93599.46 examples\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà| 14918/14918 [00:00<00:00, 92547.78 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 6127.54it/s]\nRunning generate_until requests:   0%|                    | 0/2 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:27<00:00, 13.70s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n|     Tasks      |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|----------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_python|      1|none  |None  |smoothed_bleu_4|1.195|¬±  |0.2819|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_python/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_python/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_python/alias code2text_python\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_python/smoothed_bleu_4 1.19499\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_python/smoothed_bleu_4_stderr 0.28195\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlemon-pie-82\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/twm0h6ct\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_140209-twm0h6ct/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_ruby","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks code2text_ruby \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 2 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:06:53.898784Z","iopub.execute_input":"2024-03-14T14:06:53.899097Z","iopub.status.idle":"2024-03-14T14:09:14.123614Z","shell.execute_reply.started":"2024-03-14T14:06:53.899066Z","shell.execute_reply":"2024-03-14T14:09:14.122553Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2024-03-14 14:06:58.854281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:06:58.854341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:06:58.855826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_140708-v8y6l80a\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcustard-brulee-83\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/v8y6l80a\u001b[0m\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 906/906 [00:00<00:00, 2.43MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.8M/19.8M [00:05<00:00, 3.94MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.06M/1.06M [00:01<00:00, 910kB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.03M/1.03M [00:01<00:00, 979kB/s]\nGenerating train split: 100%|‚ñà‚ñà| 24927/24927 [00:00<00:00, 144944.97 examples/s]\nGenerating validation split: 100%|‚ñà| 1400/1400 [00:00<00:00, 117572.19 examples/\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1261/1261 [00:00<00:00, 99819.15 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 4832.15it/s]\nRunning generate_until requests:   0%|                    | 0/2 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  9.17s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n|    Tasks     |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|--------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_ruby|      3|none  |None  |smoothed_bleu_4|    0|¬±  |     0|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_ruby/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_ruby/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_ruby/alias code2text_ruby\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_ruby/smoothed_bleu_4 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_ruby/smoothed_bleu_4_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcustard-brulee-83\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/v8y6l80a\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_140708-v8y6l80a/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### codexglue_code2text","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n    --tasks codexglue_code2text \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b \\\n    --limit 2 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:09:14.125262Z","iopub.execute_input":"2024-03-14T14:09:14.125662Z","iopub.status.idle":"2024-03-14T14:18:33.856903Z","shell.execute_reply.started":"2024-03-14T14:09:14.125601Z","shell.execute_reply":"2024-03-14T14:18:33.855732Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"2024-03-14 14:09:18.976226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:09:18.976283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:09:18.977737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_140926-f4xxwj82\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-bun-84\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/f4xxwj82\u001b[0m\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 4951.95it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7996.77it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7767.23it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7206.71it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7175.88it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 6814.47it/s]\nRunning generate_until requests:   0%|                   | 0/12 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:55<00:00,  9.65s/it]\nhf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n|         Tasks         |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|-----------------------|-------|------|------|---------------|-----:|---|-----:|\n|codexglue_code2text    |N/A    |none  |None  |smoothed_bleu_4|0.8767|¬±  |0.0592|\n| - code2text_go        |      1|none  |None  |smoothed_bleu_4|1.0054|¬±  |0.0983|\n| - code2text_java      |      1|none  |None  |smoothed_bleu_4|1.2158|¬±  |0.1657|\n| - code2text_javascript|      1|none  |None  |smoothed_bleu_4|0.8560|¬±  |0.0429|\n| - code2text_php       |      1|none  |None  |smoothed_bleu_4|0.9879|¬±  |0.0887|\n| - code2text_python    |      1|none  |None  |smoothed_bleu_4|1.1950|¬±  |0.2819|\n| - code2text_ruby      |      3|none  |None  |smoothed_bleu_4|0.0000|¬±  |0.0000|\n\n|      Groups       |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|-------------------|-------|------|------|---------------|-----:|---|-----:|\n|codexglue_code2text|N/A    |none  |None  |smoothed_bleu_4|0.8767|¬±  |0.0592|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                          code2text_go/alias  - code2text_go\n\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 1.00535\n\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr 0.0983\n\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_java/alias  - code2text_java\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 1.21576\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr 0.16566\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias  - code2text_javascr...\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 0.85602\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 0.04288\n\u001b[34m\u001b[1mwandb\u001b[0m:                         code2text_php/alias  - code2text_php\n\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 0.9879\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr 0.08867\n\u001b[34m\u001b[1mwandb\u001b[0m:                      code2text_python/alias  - code2text_python\n\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 1.19499\n\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr 0.28195\n\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_ruby/alias  - code2text_ruby\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:                   codexglue_code2text/alias codexglue_code2text\n\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 0.87667\n\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr 0.05923\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlemon-bun-84\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/f4xxwj82\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_140926-f4xxwj82/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run evaluations on starcoderbase-1b quantized model","metadata":{}},{"cell_type":"code","source":"!pip install -e .[gptq]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:18:33.859978Z","iopub.execute_input":"2024-03-14T14:18:33.860405Z","iopub.status.idle":"2024-03-14T14:19:37.814519Z","shell.execute_reply.started":"2024-03-14T14:18:33.860354Z","shell.execute_reply":"2024-03-14T14:19:37.813540Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.27.2)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.4.1)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.18.0)\nRequirement already satisfied: jsonlines in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (4.0.0)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.9.0)\nRequirement already satisfied: peft>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.9.0)\nRequirement already satisfied: pybind11>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.11.1)\nRequirement already satisfied: pytablewriter in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (1.2.0)\nRequirement already satisfied: rouge-score>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.1.2)\nRequirement already satisfied: sacrebleu>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.4.1)\nRequirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (1.2.2)\nRequirement already satisfied: sqlitedict in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.1.0)\nRequirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.1.2)\nRequirement already satisfied: tqdm-multiprocess in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.0.11)\nRequirement already satisfied: transformers>=4.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (4.38.1)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.22.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.3.8)\nRequirement already satisfied: word2number in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (1.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (10.2.0)\nCollecting auto-gptq>=0.6.0 (from auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.4.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq>=0.6.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1) (0.2.0)\nCollecting rouge (from auto-gptq>=0.6.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq>=0.6.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading gekko-1.0.7-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq>=0.6.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1) (4.66.1)\nCollecting triton==2.0.0 (from auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nCollecting cmake (from triton==2.0.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1) (3.13.1)\nCollecting lit (from triton==2.0.0->auto-gptq[triton]>=0.6.0; extra == \"gptq\"->lm_eval==0.4.1)\n  Downloading lit-18.1.1.tar.gz (161 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (15.0.1)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (0.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->lm_eval==0.4.1) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.16.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (2.8.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (5.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.1.2)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==0.4.1) (0.15.2)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->lm_eval==0.4.1) (23.2.0)\nRequirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (69.0.3)\nRequirement already satisfied: DataProperty<2,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (1.0.1)\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (1.1.3)\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (3.2.0)\nRequirement already satisfied: tabledata<2,>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (1.3.3)\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (0.1.4)\nRequirement already satisfied: typepy<2,>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (1.3.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (4.0.3)\nRequirement already satisfied: chardet<6,>=3.0.4 in /opt/conda/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.1) (5.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.21.0->lm_eval==0.4.1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2.8.2)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2023.3.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==0.4.1) (2.1.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.1) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==0.4.1) (1.3.0)\nDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gekko-1.0.7-py3-none-any.whl (13.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: lm_eval, lit\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.1-0.editable-py3-none-any.whl size=15041 sha256=dbd31ef369185abb30b5d205764292d8148bac1a8cebb8afa37368f3ec223063\n  Stored in directory: /tmp/pip-ephem-wheel-cache-3f2iqgfd/wheels/1b/1a/1b/44c80ddb18c9d7d3ce79a8d6d4561bddaddcbffb4cdfbf3259\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-18.1.1-py3-none-any.whl size=96363 sha256=7699f02b1d9a5c870e992ff2c0a72cadee18ef1374934a1e6a45d2e346b723e6\n  Stored in directory: /root/.cache/pip/wheels/1d/74/6b/88e95944e9f9078f1dc1c0f634a542efb4d26ecae6000ca8cf\nSuccessfully built lm_eval lit\nInstalling collected packages: lit, cmake, rouge, gekko, triton, auto-gptq, lm_eval\n  Attempting uninstall: lm_eval\n    Found existing installation: lm_eval 0.4.1\n    Uninstalling lm_eval-0.4.1:\n      Successfully uninstalled lm_eval-0.4.1\nSuccessfully installed auto-gptq-0.7.1 cmake-3.28.3 gekko-1.0.7 lit-18.1.1 lm_eval-0.4.1 rouge-1.0.1 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### bigbench_code_line_description_generate_until","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks bigbench_code_line_description_generate_until \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:19:37.816077Z","iopub.execute_input":"2024-03-14T14:19:37.816450Z","iopub.status.idle":"2024-03-14T14:23:29.932610Z","shell.execute_reply.started":"2024-03-14T14:19:37.816406Z","shell.execute_reply":"2024-03-14T14:23:29.931008Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"2024-03-14 14:19:43.326599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:19:43.326661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:19:43.328208: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_141951-2pzbd0v8\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhazelnut-bun-85\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/2pzbd0v8\u001b[0m\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.33k/1.33k [00:00<00:00, 3.36MB/s]\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nquantize_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 266/266 [00:00<00:00, 689kB/s]\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\ngptq_model-4bit-128g.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 968M/968M [00:17<00:00, 56.3MB/s]\nINFO - The layer lm_head is not quantized.\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.08k/4.08k [00:00<00:00, 9.20MB/s]\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 777k/777k [00:00<00:00, 2.37MB/s]\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 442k/442k [00:00<00:00, 1.82MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.06M/2.06M [00:00<00:00, 4.96MB/s]\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 874/874 [00:00<00:00, 2.44MB/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 40168.91it/s]\nRunning generate_until requests:   0%|                   | 0/60 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [01:45<00:00,  1.76s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|                    Tasks                    |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n|---------------------------------------------|------:|------|------|-----------|----:|---|-----:|\n|bigbench_code_line_description_generate_until|      1|none  |None  |exact_match|    0|¬±  |     0|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:              bigbench_code_line_description_generate_until/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mhazelnut-bun-85\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/2pzbd0v8\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_141951-2pzbd0v8/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### bigbench_code_line_description_multiple_choice","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks bigbench_code_line_description_multiple_choice \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:23:29.934233Z","iopub.execute_input":"2024-03-14T14:23:29.934594Z","iopub.status.idle":"2024-03-14T14:25:38.429948Z","shell.execute_reply.started":"2024-03-14T14:23:29.934549Z","shell.execute_reply":"2024-03-14T14:25:38.428706Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"2024-03-14 14:23:34.740212: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:23:34.740270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:23:34.741743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_142341-31p60px3\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmaple-brownie-86\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/31p60px3\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 1929.11it/s]\nRunning loglikelihood requests:   0%|                   | 0/242 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\nDetermined largest batch size: 64\nRunning loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 242/242 [00:11<00:00, 20.17it/s]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n|                    Tasks                     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n|----------------------------------------------|------:|------|------|------|-----:|---|-----:|\n|bigbench_code_line_description_multiple_choice|      0|none  |None  |acc   |0.1333|¬±  |0.0443|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc 0.13333\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr 0.04426\n\u001b[34m\u001b[1mwandb\u001b[0m:      bigbench_code_line_description_multiple_choice/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmaple-brownie-86\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/31p60px3\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_142341-31p60px3/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_go","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_go \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:25:38.431542Z","iopub.execute_input":"2024-03-14T14:25:38.431856Z","iopub.status.idle":"2024-03-14T14:31:38.854207Z","shell.execute_reply.started":"2024-03-14T14:25:38.431824Z","shell.execute_reply":"2024-03-14T14:31:38.853200Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"2024-03-14 14:25:43.302006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:25:43.302066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:25:43.303838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_142550-owfao5af\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpecan-crumble-87\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/owfao5af\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 13783.45it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:19<00:00, 19.96s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|   Tasks    |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_go|      1|none  |None  |smoothed_bleu_4|1.1338|¬±  |0.0439|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_go/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_go/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_go/alias code2text_go\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_go/smoothed_bleu_4 1.13379\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_go/smoothed_bleu_4_stderr 0.04385\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mpecan-crumble-87\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/owfao5af\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_142550-owfao5af/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_java","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_java \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:31:38.857959Z","iopub.execute_input":"2024-03-14T14:31:38.858360Z","iopub.status.idle":"2024-03-14T14:37:42.103027Z","shell.execute_reply.started":"2024-03-14T14:31:38.858318Z","shell.execute_reply":"2024-03-14T14:37:42.101907Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"2024-03-14 14:31:44.073952: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:31:44.074016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:31:44.075496: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_143151-10a0xwtx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhershey-pie-88\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/10a0xwtx\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 15756.21it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:22<00:00, 20.28s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|    Tasks     |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|--------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_java|      1|none  |None  |smoothed_bleu_4|1.0605|¬±  |0.1231|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_java/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_java/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_java/alias code2text_java\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_java/smoothed_bleu_4 1.06048\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_java/smoothed_bleu_4_stderr 0.12307\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mhershey-pie-88\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/10a0xwtx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_143151-10a0xwtx/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_javascript","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_javascript \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:37:42.104732Z","iopub.execute_input":"2024-03-14T14:37:42.105143Z","iopub.status.idle":"2024-03-14T14:43:05.839778Z","shell.execute_reply.started":"2024-03-14T14:37:42.105100Z","shell.execute_reply":"2024-03-14T14:43:05.838527Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"2024-03-14 14:37:47.172642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:37:47.172727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:37:47.174239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_143754-p617jt2l\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mneapolitan-meringue-89\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/p617jt2l\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 13565.02it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:18<00:00, 19.86s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|       Tasks        |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|--------------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_javascript|      1|none  |None  |smoothed_bleu_4|0.9016|¬±  |0.1405|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias code2text_javascript...\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 0.90157\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 0.14052\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mneapolitan-meringue-89\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/p617jt2l\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_143754-p617jt2l/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_php","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_php \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 8 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:43:05.841425Z","iopub.execute_input":"2024-03-14T14:43:05.841773Z","iopub.status.idle":"2024-03-14T14:49:04.124981Z","shell.execute_reply.started":"2024-03-14T14:43:05.841738Z","shell.execute_reply":"2024-03-14T14:49:04.123659Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"2024-03-14 14:43:10.971202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:43:10.971277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:43:10.972892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_144318-s80zbmgh\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melderberry-flambee-90\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/s80zbmgh\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12782.64it/s]\nRunning generate_until requests:   0%|                    | 0/8 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [02:51<00:00, 21.42s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 8.0, num_fewshot: None, batch_size: auto\n|    Tasks    |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|-------------|------:|------|------|---------------|-----:|---|-----:|\n|code2text_php|      1|none  |None  |smoothed_bleu_4|0.9075|¬±  |0.1519|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_php/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_php/alias code2text_php\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4 0.90749\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_php/smoothed_bleu_4_stderr 0.15194\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33melderberry-flambee-90\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/s80zbmgh\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_144318-s80zbmgh/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_python","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_python \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 2 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:49:04.126750Z","iopub.execute_input":"2024-03-14T14:49:04.127092Z","iopub.status.idle":"2024-03-14T14:53:18.776388Z","shell.execute_reply.started":"2024-03-14T14:49:04.127058Z","shell.execute_reply":"2024-03-14T14:53:18.775211Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"2024-03-14 14:49:09.041474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:49:09.041536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:49:09.043305: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_144916-3ary65tv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheese-flambee-91\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/3ary65tv\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 5346.47it/s]\nRunning generate_until requests:   0%|                    | 0/2 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:07<00:00, 33.75s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n|     Tasks      |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|----------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_python|      1|none  |None  |smoothed_bleu_4|1.146|¬±  | 1.146|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_python/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_python/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_python/alias code2text_python\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_python/smoothed_bleu_4 1.14595\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_python/smoothed_bleu_4_stderr 1.14595\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcheese-flambee-91\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/3ary65tv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_144916-3ary65tv/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### code2text_ruby","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks code2text_ruby \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 4 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:53:18.778004Z","iopub.execute_input":"2024-03-14T14:53:18.778347Z","iopub.status.idle":"2024-03-14T14:56:43.354259Z","shell.execute_reply.started":"2024-03-14T14:53:18.778313Z","shell.execute_reply":"2024-03-14T14:56:43.353146Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"2024-03-14 14:53:23.603042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:53:23.603102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:53:23.604599: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_145330-dfelctn7\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcookie-strudel-92\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/dfelctn7\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 11275.01it/s]\nRunning generate_until requests:   0%|                    | 0/4 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:34<00:00, 23.63s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 4.0, num_fewshot: None, batch_size: auto\n|    Tasks     |Version|Filter|n-shot|    Metric     |Value|   |Stderr|\n|--------------|------:|------|------|---------------|----:|---|-----:|\n|code2text_ruby|      3|none  |None  |smoothed_bleu_4|    0|¬±  |     0|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_ruby/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_ruby/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_ruby/alias code2text_ruby\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_ruby/smoothed_bleu_4 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_ruby/smoothed_bleu_4_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcookie-strudel-92\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/dfelctn7\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_145330-dfelctn7/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### codexglue_code2text","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks codexglue_code2text \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-1b-GPTQ \\\n    --limit 2 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:56:43.355824Z","iopub.execute_input":"2024-03-14T14:56:43.356156Z","iopub.status.idle":"2024-03-14T15:08:21.058405Z","shell.execute_reply.started":"2024-03-14T14:56:43.356124Z","shell.execute_reply":"2024-03-14T15:08:21.057354Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"2024-03-14 14:56:48.368390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 14:56:48.368455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 14:56:48.370057: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240314_145655-x8hhqvos\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mderby-mousse-93\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/x8hhqvos\u001b[0m\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 4720.66it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 8168.07it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7358.43it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 6141.00it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 7523.42it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 5903.31it/s]\nRunning generate_until requests:   0%|                   | 0/12 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [04:22<00:00, 21.83s/it]\nhf (pretrained=cosmo3769/starcoderbase-1b-GPTQ,autogptq=True,gptq_use_triton=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n|         Tasks         |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|-----------------------|-------|------|------|---------------|-----:|---|-----:|\n|codexglue_code2text    |N/A    |none  |None  |smoothed_bleu_4|0.7959|¬±  |0.2180|\n| - code2text_go        |      1|none  |None  |smoothed_bleu_4|0.9280|¬±  |0.0291|\n| - code2text_java      |      1|none  |None  |smoothed_bleu_4|1.2112|¬±  |0.1703|\n| - code2text_javascript|      1|none  |None  |smoothed_bleu_4|0.8848|¬±  |0.0391|\n| - code2text_php       |      1|none  |None  |smoothed_bleu_4|0.6055|¬±  |0.6055|\n| - code2text_python    |      1|none  |None  |smoothed_bleu_4|1.1460|¬±  |1.1460|\n| - code2text_ruby      |      3|none  |None  |smoothed_bleu_4|0.0000|¬±  |0.0000|\n\n|      Groups       |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n|-------------------|-------|------|------|---------------|-----:|---|-----:|\n|codexglue_code2text|N/A    |none  |None  |smoothed_bleu_4|0.7959|¬±  | 0.218|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                          code2text_go/alias  - code2text_go\n\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 0.92802\n\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr 0.02911\n\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_java/alias  - code2text_java\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 1.21117\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr 0.17026\n\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias  - code2text_javascr...\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 0.88475\n\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 0.03906\n\u001b[34m\u001b[1mwandb\u001b[0m:                         code2text_php/alias  - code2text_php\n\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 0.60546\n\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr 0.60546\n\u001b[34m\u001b[1mwandb\u001b[0m:                      code2text_python/alias  - code2text_python\n\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 1.14595\n\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr 1.14595\n\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_ruby/alias  - code2text_ruby\n\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:                   codexglue_code2text/alias codexglue_code2text\n\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 0.79589\n\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr 0.21802\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mderby-mousse-93\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/x8hhqvos\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240314_145655-x8hhqvos/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}