{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e053b85180124ce9a5bac5859ce12078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86f42c928f12401faddd40522373f698",
              "IPY_MODEL_2edb4729e29549a2987c51368eac8dc4",
              "IPY_MODEL_8098a2bdd5d44828821c0bded96cc99c"
            ],
            "layout": "IPY_MODEL_b47e9b29e97c433cbf1eb949a39b3063"
          }
        },
        "86f42c928f12401faddd40522373f698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3807a80d575e4cd4932a4adcdfd8c77d",
            "placeholder": "​",
            "style": "IPY_MODEL_fea388b6cb534b3e9d3f8ec1964eca9f",
            "value": "starcoderbase-3b.Q4_K_M.gguf: 100%"
          }
        },
        "2edb4729e29549a2987c51368eac8dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98592ed935194b5db999b25fd58fb30a",
            "max": 2050319168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b15b2a75d6e4bf8b55bb6ece1da089d",
            "value": 2050319168
          }
        },
        "8098a2bdd5d44828821c0bded96cc99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1045d0335a3c4cbc9bb28f157356aa74",
            "placeholder": "​",
            "style": "IPY_MODEL_ba4ee6178a1043c0b8a40c283f04a322",
            "value": " 2.05G/2.05G [00:54&lt;00:00, 34.1MB/s]"
          }
        },
        "b47e9b29e97c433cbf1eb949a39b3063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3807a80d575e4cd4932a4adcdfd8c77d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea388b6cb534b3e9d3f8ec1964eca9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98592ed935194b5db999b25fd58fb30a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b15b2a75d6e4bf8b55bb6ece1da089d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1045d0335a3c4cbc9bb28f157356aa74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4ee6178a1043c0b8a40c283f04a322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmo3769/Quantized-LLMs/blob/main/notebooks/quantize_starcoderbase_3b_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install llama.cpp"
      ],
      "metadata": {
        "id": "8MWEm0aE6fj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rHpVkDZQ1MdH",
        "outputId": "a999234f-3b7d-4860-a069-049ac8ff178f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19631, done.\u001b[K\n",
            "remote: Counting objects: 100% (6945/6945), done.\u001b[K\n",
            "remote: Compressing objects: 100% (543/543), done.\u001b[K\n",
            "remote: Total 19631 (delta 6706), reused 6482 (delta 6400), pack-reused 12686\u001b[K\n",
            "Receiving objects: 100% (19631/19631), 23.48 MiB | 22.54 MiB/s, done.\n",
            "Resolving deltas: 100% (13875/13875), done.\n",
            "Already up to date.\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include \n",
            "I NVCCFLAGS: -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
            "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I NVCC:      Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/console.cpp -o console.o\n",
            "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda.cu -o ggml-cuda.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/main/main.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize/quantize.o -o quantize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize-stats/quantize-stats.o -o quantize-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/perplexity/perplexity.o -o perplexity -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/imatrix/imatrix.o -o imatrix -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/embedding/embedding.o -o embedding -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/vdot.o -o vdot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/q8dot.o -o q8dot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/simple/simple.o -o simple -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched/batched.o -o batched -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched-bench/batched-bench.o -o batched-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/save-load-state/save-load-state.o -o save-load-state -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/clip.cpp -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Iexamples/server examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/server/server.o examples/llava/clip.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/gguf/gguf.o -o gguf -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llama-bench/llama-bench.o -o llama-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/baby-llama/baby-llama.o -o baby-llama -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/beam-search/beam-search.o -o beam-search -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/speculative/speculative.o -o speculative -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/infill/infill.o -o infill -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/tokenize/tokenize.o -o tokenize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/parallel/parallel.o -o parallel -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/finetune/finetune.o -o finetune -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/export-lora/export-lora.o -o export-lora -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookahead/lookahead.o -o lookahead -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookup/lookup.o -o lookup -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/passkey/passkey.o -o passkey -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops~=0.7.0 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d26fac830ed3456ea9af76773cb54b15"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "WL6JFw1SAgvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose Model"
      ],
      "metadata": {
        "id": "jelBX90p6jWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"bigcode/starcoderbase-3b\"\n",
        "QUANTIZATION_METHOD = \"q4_k_m\"\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]"
      ],
      "metadata": {
        "id": "dmNTS8PF2bze"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model"
      ],
      "metadata": {
        "id": "2DtVazNg6mOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "_12eBfY0sJki"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmduM-uJJT0S",
        "outputId": "b6f759db-6af6-45ef-f919-309fe8fbc74c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmpfaT42hzk",
        "outputId": "7a4c205d-3b9a-4654-b638-74922d768935"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'starcoderbase-3b'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Total 23 (delta 0), reused 0 (delta 0), pack-reused 23\u001b[K\n",
            "Unpacking objects: 100% (23/23), 1.12 MiB | 7.97 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.33 GiB | 4.97 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Model"
      ],
      "metadata": {
        "id": "8bkR96lx6qzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4DAECO42wob",
        "outputId": "e3c781d2-9503-4f72-deef-aa48e40134d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: starcoderbase-3b\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 48891 merge(s).\n",
            "gguf: Setting special token type bos to 0\n",
            "gguf: Setting special token type eos to 0\n",
            "gguf: Setting special token type unk to 0\n",
            "Exporting model to 'starcoderbase-3b/starcoderbase-3b.fp16.bin'\n",
            "gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "token_embd.weight, n_dims = 2, torch.float32 --> float16\n",
            "position_embd.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.24.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.24.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.24.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.24.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.24.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.25.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.25.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.25.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.25.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.25.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.26.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.26.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.26.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.26.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.26.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.27.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.27.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.27.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.27.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.27.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.28.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.28.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.28.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.28.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.28.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.29.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.29.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "output.weight, n_dims = 2, torch.float32 --> float16\n",
            "gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "blk.29.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.29.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.29.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.29.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.30.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.30.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.30.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.30.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.30.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.31.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.31.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.31.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.31.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.31.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.32.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.32.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.32.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.32.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.32.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.33.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.33.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.33.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.33.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.33.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.34.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.34.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.34.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.34.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.34.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.attn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.attn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.attn_qkv.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.35.attn_qkv.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.attn_output.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.35.attn_output.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.ffn_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.ffn_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.ffn_up.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.35.ffn_up.bias, n_dims = 1, torch.float32 --> float32\n",
            "blk.35.ffn_down.weight, n_dims = 2, torch.float32 --> float16\n",
            "blk.35.ffn_down.bias, n_dims = 1, torch.float32 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.float32 --> float32\n",
            "output_norm.bias, n_dims = 1, torch.float32 --> float32\n",
            "Model successfully exported to 'starcoderbase-3b/starcoderbase-3b.fp16.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model\n",
        "qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "!./llama.cpp/quantize {fp16} {qtype} {QUANTIZATION_METHOD}"
      ],
      "metadata": {
        "id": "e8rSb-jjuVZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0dfe29e-cb8c-46e2-aeb1-8028ea584f82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 2333 (a0fc6266)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'starcoderbase-3b/starcoderbase-3b.fp16.bin' to 'starcoderbase-3b/starcoderbase-3b.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 17 key-value pairs and 437 tensors from starcoderbase-3b/starcoderbase-3b.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = starcoder\n",
            "llama_model_loader: - kv   1:                               general.name str              = StarCoder\n",
            "llama_model_loader: - kv   2:                   starcoder.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                 starcoder.embedding_length u32              = 2816\n",
            "llama_model_loader: - kv   4:              starcoder.feed_forward_length u32              = 11264\n",
            "llama_model_loader: - kv   5:                      starcoder.block_count u32              = 36\n",
            "llama_model_loader: - kv   6:             starcoder.attention.head_count u32              = 22\n",
            "llama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - type  f32:  290 tensors\n",
            "llama_model_loader: - type  f16:  147 tensors\n",
            "llama_model_quantize_internal ============ Strange model: n_attention_wv = 36, n_ffn_down = 72, hparams.n_layer = 36\n",
            "llama_model_quantize_internal: meta size = 1743680 bytes\n",
            "[   1/ 437]                    token_embd.weight - [ 2816, 49152,     1,     1], type =    f16, quantizing to q4_K .. size =   264.00 MiB ->    74.25 MiB\n",
            "[   2/ 437]                 position_embd.weight - [ 2816,  8192,     1,     1], type =    f16, size =   44.000 MB\n",
            "[   3/ 437]               blk.0.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[   4/ 437]                 blk.0.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[   5/ 437]                blk.0.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[   6/ 437]                  blk.0.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   7/ 437]             blk.0.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[   8/ 437]               blk.0.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[   9/ 437]                blk.0.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  10/ 437]                  blk.0.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  11/ 437]                  blk.0.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  12/ 437]                    blk.0.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  13/ 437]                blk.0.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  14/ 437]                  blk.0.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  15/ 437]               blk.1.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  16/ 437]                 blk.1.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  17/ 437]                blk.1.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  18/ 437]                  blk.1.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  19/ 437]             blk.1.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  20/ 437]               blk.1.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  21/ 437]                blk.1.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  22/ 437]                  blk.1.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  23/ 437]                  blk.1.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  24/ 437]                    blk.1.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  25/ 437]                blk.1.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  26/ 437]                  blk.1.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  27/ 437]               blk.2.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  28/ 437]                 blk.2.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  29/ 437]                blk.2.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  30/ 437]                  blk.2.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  31/ 437]             blk.2.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  32/ 437]               blk.2.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  33/ 437]                blk.2.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  34/ 437]                  blk.2.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  35/ 437]                  blk.2.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  36/ 437]                    blk.2.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  37/ 437]                blk.2.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  38/ 437]                  blk.2.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  39/ 437]               blk.3.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  40/ 437]                 blk.3.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  41/ 437]                blk.3.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  42/ 437]                  blk.3.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  43/ 437]             blk.3.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  44/ 437]               blk.3.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  45/ 437]                blk.3.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  46/ 437]                  blk.3.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  47/ 437]                  blk.3.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  48/ 437]                    blk.3.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  49/ 437]                blk.3.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  50/ 437]                  blk.3.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  51/ 437]               blk.4.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  52/ 437]                 blk.4.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  53/ 437]                blk.4.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  54/ 437]                  blk.4.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  55/ 437]             blk.4.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  56/ 437]               blk.4.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  57/ 437]                blk.4.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  58/ 437]                  blk.4.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  59/ 437]                  blk.4.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  60/ 437]                    blk.4.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  61/ 437]                blk.4.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  62/ 437]                  blk.4.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  63/ 437]               blk.5.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  64/ 437]                 blk.5.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  65/ 437]                blk.5.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  66/ 437]                  blk.5.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  67/ 437]             blk.5.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  68/ 437]               blk.5.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  69/ 437]                blk.5.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  70/ 437]                  blk.5.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  71/ 437]                  blk.5.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  72/ 437]                    blk.5.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  73/ 437]                blk.5.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  74/ 437]                  blk.5.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  75/ 437]               blk.6.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  76/ 437]                 blk.6.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  77/ 437]                blk.6.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  78/ 437]                  blk.6.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  79/ 437]             blk.6.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  80/ 437]               blk.6.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  81/ 437]                blk.6.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  82/ 437]                  blk.6.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  83/ 437]                  blk.6.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  84/ 437]                    blk.6.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  85/ 437]                blk.6.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  86/ 437]                  blk.6.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  87/ 437]               blk.7.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  88/ 437]                 blk.7.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  89/ 437]                blk.7.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[  90/ 437]                  blk.7.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  91/ 437]             blk.7.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[  92/ 437]               blk.7.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  93/ 437]                blk.7.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  94/ 437]                  blk.7.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  95/ 437]                  blk.7.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[  96/ 437]                    blk.7.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[  97/ 437]                blk.7.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[  98/ 437]                  blk.7.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[  99/ 437]               blk.8.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 100/ 437]                 blk.8.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 101/ 437]                blk.8.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 102/ 437]                  blk.8.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 103/ 437]             blk.8.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 104/ 437]               blk.8.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 105/ 437]                blk.8.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 106/ 437]                  blk.8.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 107/ 437]                  blk.8.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 108/ 437]                    blk.8.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 109/ 437]                blk.8.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 110/ 437]                  blk.8.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 111/ 437]               blk.9.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 112/ 437]                 blk.9.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 113/ 437]                blk.9.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 114/ 437]                  blk.9.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 115/ 437]             blk.9.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 116/ 437]               blk.9.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 117/ 437]                blk.9.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 118/ 437]                  blk.9.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 119/ 437]                  blk.9.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 120/ 437]                    blk.9.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 121/ 437]                blk.9.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 122/ 437]                  blk.9.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 123/ 437]              blk.10.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 124/ 437]                blk.10.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 125/ 437]               blk.10.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 126/ 437]                 blk.10.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 127/ 437]            blk.10.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 128/ 437]              blk.10.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 129/ 437]               blk.10.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 130/ 437]                 blk.10.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 131/ 437]                 blk.10.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 132/ 437]                   blk.10.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 133/ 437]               blk.10.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 134/ 437]                 blk.10.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 135/ 437]              blk.11.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 136/ 437]                blk.11.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 137/ 437]               blk.11.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 138/ 437]                 blk.11.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 139/ 437]            blk.11.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 140/ 437]              blk.11.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 141/ 437]               blk.11.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 142/ 437]                 blk.11.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 143/ 437]                 blk.11.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 144/ 437]                   blk.11.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 145/ 437]               blk.11.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 146/ 437]                 blk.11.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 147/ 437]              blk.12.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 148/ 437]                blk.12.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 149/ 437]               blk.12.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 150/ 437]                 blk.12.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 151/ 437]            blk.12.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 152/ 437]              blk.12.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 153/ 437]               blk.12.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 154/ 437]                 blk.12.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 155/ 437]                 blk.12.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 156/ 437]                   blk.12.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 157/ 437]               blk.12.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 158/ 437]                 blk.12.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 159/ 437]              blk.13.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 160/ 437]                blk.13.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 161/ 437]               blk.13.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 162/ 437]                 blk.13.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 163/ 437]            blk.13.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 164/ 437]              blk.13.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 165/ 437]               blk.13.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 166/ 437]                 blk.13.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 167/ 437]                 blk.13.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 168/ 437]                   blk.13.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 169/ 437]               blk.13.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 170/ 437]                 blk.13.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 171/ 437]              blk.14.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 172/ 437]                blk.14.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 173/ 437]               blk.14.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 174/ 437]                 blk.14.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 175/ 437]            blk.14.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 176/ 437]              blk.14.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 177/ 437]               blk.14.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 178/ 437]                 blk.14.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 179/ 437]                 blk.14.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 180/ 437]                   blk.14.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 181/ 437]               blk.14.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 182/ 437]                 blk.14.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 183/ 437]              blk.15.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 184/ 437]                blk.15.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 185/ 437]               blk.15.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 186/ 437]                 blk.15.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 187/ 437]            blk.15.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 188/ 437]              blk.15.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 189/ 437]               blk.15.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 190/ 437]                 blk.15.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 191/ 437]                 blk.15.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 192/ 437]                   blk.15.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 193/ 437]               blk.15.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 194/ 437]                 blk.15.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 195/ 437]              blk.16.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 196/ 437]                blk.16.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 197/ 437]               blk.16.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 198/ 437]                 blk.16.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 199/ 437]            blk.16.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 200/ 437]              blk.16.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 201/ 437]               blk.16.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 202/ 437]                 blk.16.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 203/ 437]                 blk.16.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 204/ 437]                   blk.16.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 205/ 437]               blk.16.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 206/ 437]                 blk.16.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 207/ 437]              blk.17.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 208/ 437]                blk.17.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 209/ 437]               blk.17.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 210/ 437]                 blk.17.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 211/ 437]            blk.17.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 212/ 437]              blk.17.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 213/ 437]               blk.17.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 214/ 437]                 blk.17.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 215/ 437]                 blk.17.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 216/ 437]                   blk.17.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 217/ 437]               blk.17.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 218/ 437]                 blk.17.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 219/ 437]              blk.18.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 220/ 437]                blk.18.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 221/ 437]               blk.18.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 222/ 437]                 blk.18.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 223/ 437]            blk.18.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 224/ 437]              blk.18.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 225/ 437]               blk.18.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 226/ 437]                 blk.18.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 227/ 437]                 blk.18.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 228/ 437]                   blk.18.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 229/ 437]               blk.18.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 230/ 437]                 blk.18.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 231/ 437]              blk.19.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 232/ 437]                blk.19.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 233/ 437]               blk.19.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 234/ 437]                 blk.19.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 235/ 437]            blk.19.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 236/ 437]              blk.19.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 237/ 437]               blk.19.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 238/ 437]                 blk.19.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 239/ 437]                 blk.19.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 240/ 437]                   blk.19.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 241/ 437]               blk.19.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 242/ 437]                 blk.19.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 243/ 437]              blk.20.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 244/ 437]                blk.20.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 245/ 437]               blk.20.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 246/ 437]                 blk.20.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 247/ 437]            blk.20.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 248/ 437]              blk.20.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 249/ 437]               blk.20.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 250/ 437]                 blk.20.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 251/ 437]                 blk.20.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 252/ 437]                   blk.20.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 253/ 437]               blk.20.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 254/ 437]                 blk.20.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 255/ 437]              blk.21.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 256/ 437]                blk.21.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 257/ 437]               blk.21.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 258/ 437]                 blk.21.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 259/ 437]            blk.21.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 260/ 437]              blk.21.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 261/ 437]               blk.21.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 262/ 437]                 blk.21.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 263/ 437]                 blk.21.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 264/ 437]                   blk.21.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 265/ 437]               blk.21.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 266/ 437]                 blk.21.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 267/ 437]              blk.22.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 268/ 437]                blk.22.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 269/ 437]               blk.22.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 270/ 437]                 blk.22.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 271/ 437]            blk.22.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 272/ 437]              blk.22.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 273/ 437]               blk.22.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 274/ 437]                 blk.22.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 275/ 437]                 blk.22.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 276/ 437]                   blk.22.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 277/ 437]               blk.22.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 278/ 437]                 blk.22.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 279/ 437]              blk.23.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 280/ 437]                blk.23.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 281/ 437]               blk.23.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 282/ 437]                 blk.23.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 283/ 437]            blk.23.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 284/ 437]              blk.23.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 285/ 437]               blk.23.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 286/ 437]                 blk.23.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 287/ 437]                 blk.23.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 288/ 437]                   blk.23.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 289/ 437]               blk.23.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 290/ 437]                 blk.23.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 291/ 437]              blk.24.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 292/ 437]                blk.24.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 293/ 437]               blk.24.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 294/ 437]                 blk.24.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 295/ 437]            blk.24.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 296/ 437]              blk.24.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 297/ 437]               blk.24.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 298/ 437]                 blk.24.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 299/ 437]                 blk.24.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 300/ 437]                   blk.24.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 301/ 437]               blk.24.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 302/ 437]                 blk.24.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 303/ 437]              blk.25.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 304/ 437]                blk.25.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 305/ 437]               blk.25.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 306/ 437]                 blk.25.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 307/ 437]            blk.25.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 308/ 437]              blk.25.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 309/ 437]               blk.25.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 310/ 437]                 blk.25.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 311/ 437]                 blk.25.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 312/ 437]                   blk.25.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 313/ 437]               blk.25.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 314/ 437]                 blk.25.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 315/ 437]              blk.26.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 316/ 437]                blk.26.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 317/ 437]               blk.26.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 318/ 437]                 blk.26.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 319/ 437]            blk.26.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 320/ 437]              blk.26.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 321/ 437]               blk.26.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 322/ 437]                 blk.26.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 323/ 437]                 blk.26.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 324/ 437]                   blk.26.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 325/ 437]               blk.26.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 326/ 437]                 blk.26.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 327/ 437]              blk.27.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 328/ 437]                blk.27.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 329/ 437]               blk.27.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 330/ 437]                 blk.27.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 331/ 437]            blk.27.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 332/ 437]              blk.27.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 333/ 437]               blk.27.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 334/ 437]                 blk.27.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 335/ 437]                 blk.27.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 336/ 437]                   blk.27.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 337/ 437]               blk.27.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 338/ 437]                 blk.27.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 339/ 437]              blk.28.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 340/ 437]                blk.28.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 341/ 437]               blk.28.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 342/ 437]                 blk.28.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 343/ 437]            blk.28.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 344/ 437]              blk.28.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 345/ 437]               blk.28.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 346/ 437]                 blk.28.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 347/ 437]                 blk.28.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 348/ 437]                   blk.28.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 349/ 437]               blk.28.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 350/ 437]                 blk.28.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 351/ 437]              blk.29.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 352/ 437]                blk.29.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 353/ 437]               blk.29.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 354/ 437]                 blk.29.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 355/ 437]            blk.29.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 356/ 437]              blk.29.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 357/ 437]               blk.29.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 358/ 437]                 blk.29.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 359/ 437]                        output.weight - [ 2816, 49152,     1,     1], type =    f16, quantizing to q6_K .. size =   264.00 MiB ->   108.28 MiB\n",
            "[ 360/ 437]                 blk.29.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 361/ 437]                   blk.29.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 362/ 437]               blk.29.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 363/ 437]                 blk.29.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 364/ 437]              blk.30.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 365/ 437]                blk.30.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 366/ 437]               blk.30.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 367/ 437]                 blk.30.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 368/ 437]            blk.30.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 369/ 437]              blk.30.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 370/ 437]               blk.30.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 371/ 437]                 blk.30.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 372/ 437]                 blk.30.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 373/ 437]                   blk.30.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 374/ 437]               blk.30.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 375/ 437]                 blk.30.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 376/ 437]              blk.31.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 377/ 437]                blk.31.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 378/ 437]               blk.31.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 379/ 437]                 blk.31.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 380/ 437]            blk.31.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 381/ 437]              blk.31.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 382/ 437]               blk.31.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 383/ 437]                 blk.31.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 384/ 437]                 blk.31.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 385/ 437]                   blk.31.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 386/ 437]               blk.31.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 387/ 437]                 blk.31.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 388/ 437]              blk.32.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 389/ 437]                blk.32.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 390/ 437]               blk.32.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 391/ 437]                 blk.32.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 392/ 437]            blk.32.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 393/ 437]              blk.32.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 394/ 437]               blk.32.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 395/ 437]                 blk.32.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 396/ 437]                 blk.32.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 397/ 437]                   blk.32.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 398/ 437]               blk.32.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 399/ 437]                 blk.32.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 400/ 437]              blk.33.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 401/ 437]                blk.33.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 402/ 437]               blk.33.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 403/ 437]                 blk.33.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 404/ 437]            blk.33.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 405/ 437]              blk.33.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 406/ 437]               blk.33.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 407/ 437]                 blk.33.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 408/ 437]                 blk.33.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 409/ 437]                   blk.33.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 410/ 437]               blk.33.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 411/ 437]                 blk.33.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 412/ 437]              blk.34.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 413/ 437]                blk.34.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 414/ 437]               blk.34.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 415/ 437]                 blk.34.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 416/ 437]            blk.34.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 417/ 437]              blk.34.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 418/ 437]               blk.34.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 419/ 437]                 blk.34.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 420/ 437]                 blk.34.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 421/ 437]                   blk.34.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 422/ 437]               blk.34.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 423/ 437]                 blk.34.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 424/ 437]              blk.35.attn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 425/ 437]                blk.35.attn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 426/ 437]               blk.35.attn_qkv.weight - [ 2816,  3072,     1,     1], type =    f16, quantizing to q5_K .. size =    16.50 MiB ->     5.67 MiB\n",
            "[ 427/ 437]                 blk.35.attn_qkv.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 428/ 437]            blk.35.attn_output.weight - [ 2816,  2816,     1,     1], type =    f16, quantizing to q4_K .. size =    15.12 MiB ->     4.25 MiB\n",
            "[ 429/ 437]              blk.35.attn_output.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 430/ 437]               blk.35.ffn_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 431/ 437]                 blk.35.ffn_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 432/ 437]                 blk.35.ffn_up.weight - [ 2816, 11264,     1,     1], type =    f16, quantizing to q4_K .. size =    60.50 MiB ->    17.02 MiB\n",
            "[ 433/ 437]                   blk.35.ffn_up.bias - [11264,     1,     1,     1], type =    f32, size =    0.043 MB\n",
            "[ 434/ 437]               blk.35.ffn_down.weight - [11264,  2816,     1,     1], type =    f16, quantizing to q6_K .. size =    60.50 MiB ->    24.81 MiB\n",
            "[ 435/ 437]                 blk.35.ffn_down.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 436/ 437]                   output_norm.weight - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "[ 437/ 437]                     output_norm.bias - [ 2816,     1,     1,     1], type =    f32, size =    0.011 MB\n",
            "llama_model_quantize_internal: model size  =  6070.81 MB\n",
            "llama_model_quantize_internal: quant size  =  1953.67 MB\n",
            "\n",
            "main: quantize time = 346200.22 ms\n",
            "main:    total time = 346200.22 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run inference"
      ],
      "metadata": {
        "id": "LBlRiOh76vzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UrrFMokmpyb",
        "outputId": "be417c9b-6d5f-4609-d724-8be3635f5280"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Write a Python function to add two intergers.\n",
            "Name of the model (options: starcoderbase-3b.Q4_K_M.gguf): starcoderbase-3b.Q4_K_M.gguf\n",
            "Log start\n",
            "main: build = 2333 (a0fc6266)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709543818\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 18 key-value pairs and 437 tensors from starcoderbase-3b/starcoderbase-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = starcoder\n",
            "llama_model_loader: - kv   1:                               general.name str              = StarCoder\n",
            "llama_model_loader: - kv   2:                   starcoder.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                 starcoder.embedding_length u32              = 2816\n",
            "llama_model_loader: - kv   4:              starcoder.feed_forward_length u32              = 11264\n",
            "llama_model_loader: - kv   5:                      starcoder.block_count u32              = 36\n",
            "llama_model_loader: - kv   6:             starcoder.attention.head_count u32              = 22\n",
            "llama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  290 tensors\n",
            "llama_model_loader: - type  f16:    1 tensors\n",
            "llama_model_loader: - type q4_K:   91 tensors\n",
            "llama_model_loader: - type q5_K:   36 tensors\n",
            "llama_model_loader: - type q6_K:   19 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 19/49152 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = starcoder\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 49152\n",
            "llm_load_print_meta: n_merges         = 48891\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2816\n",
            "llm_load_print_meta: n_head           = 22\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 36\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 22\n",
            "llm_load_print_meta: n_embd_k_gqa     = 128\n",
            "llm_load_print_meta: n_embd_v_gqa     = 128\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11264\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.18 B\n",
            "llm_load_print_meta: model size       = 1.91 GiB (5.15 BPW) \n",
            "llm_load_print_meta: general.name     = StarCoder\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 145 'Ä'\n",
            "llm_load_tensors: ggml ctx size =    0.33 MiB\n",
            "llm_load_tensors: offloading 35 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 35/37 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  1953.67 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1675.27 MiB\n",
            ".........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     0.25 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     8.75 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     7.51 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    34.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =   112.50 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33mWrite a Python function to add two intergers.\u001b[0m\n",
            "#  - Write a Python program that counts the number of characters in a string\n",
            "#  - Write a Python program to check whether a given variable is even or odd?\n",
            "#  - Write a Python code which will produce an output of sum of 10 numbers less than 5 using FOR LOOP statement.\n",
            "#  - Write a Python program to find all such numbers which are divisible by 7 but are not a multiple of 5, between 2000 and 3200 (both included). The numbers obtained should be printed in a comma-separated sequence on a single line.\n",
            "\n",
            "llama_print_timings:        load time =    7176.46 ms\n",
            "llama_print_timings:      sample time =     104.08 ms /   128 runs   (    0.81 ms per token,  1229.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     224.84 ms /    10 tokens (   22.48 ms per token,    44.48 tokens per second)\n",
            "llama_print_timings:        eval time =    6173.74 ms /   127 runs   (   48.61 ms per token,    20.57 tokens per second)\n",
            "llama_print_timings:       total time =    6570.23 ms /   137 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILplZkx6mqrc",
        "outputId": "64309217-7e47-43f1-ff21-29733320f5c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Write a Python function to output the fibonnaci numbers till 100.\n",
            "Name of the model (options: starcoderbase-3b.Q4_K_M.gguf): starcoderbase-3b.Q4_K_M.gguf\n",
            "Log start\n",
            "main: build = 2333 (a0fc6266)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709543850\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 18 key-value pairs and 437 tensors from starcoderbase-3b/starcoderbase-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = starcoder\n",
            "llama_model_loader: - kv   1:                               general.name str              = StarCoder\n",
            "llama_model_loader: - kv   2:                   starcoder.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                 starcoder.embedding_length u32              = 2816\n",
            "llama_model_loader: - kv   4:              starcoder.feed_forward_length u32              = 11264\n",
            "llama_model_loader: - kv   5:                      starcoder.block_count u32              = 36\n",
            "llama_model_loader: - kv   6:             starcoder.attention.head_count u32              = 22\n",
            "llama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  290 tensors\n",
            "llama_model_loader: - type  f16:    1 tensors\n",
            "llama_model_loader: - type q4_K:   91 tensors\n",
            "llama_model_loader: - type q5_K:   36 tensors\n",
            "llama_model_loader: - type q6_K:   19 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 19/49152 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = starcoder\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 49152\n",
            "llm_load_print_meta: n_merges         = 48891\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2816\n",
            "llm_load_print_meta: n_head           = 22\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 36\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 22\n",
            "llm_load_print_meta: n_embd_k_gqa     = 128\n",
            "llm_load_print_meta: n_embd_v_gqa     = 128\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11264\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.18 B\n",
            "llm_load_print_meta: model size       = 1.91 GiB (5.15 BPW) \n",
            "llm_load_print_meta: general.name     = StarCoder\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 145 'Ä'\n",
            "llm_load_tensors: ggml ctx size =    0.33 MiB\n",
            "llm_load_tensors: offloading 35 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 35/37 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  1953.67 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1675.27 MiB\n",
            ".........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     0.25 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     8.75 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     7.51 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    34.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =   112.50 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33mWrite a Python function to output the fibonnaci numbers till 100.\u001b[0m Use while loop in this problem and print the result using print() function.\n",
            "#\n",
            "# Sample Input :- [2]\n",
            "#\n",
            "# Sample Output:- [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n",
            "#\n",
            "\n",
            "\n",
            " [end of text]\n",
            "\n",
            "llama_print_timings:        load time =     699.45 ms\n",
            "llama_print_timings:      sample time =      75.41 ms /    97 runs   (    0.78 ms per token,  1286.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     390.30 ms /    18 tokens (   21.68 ms per token,    46.12 tokens per second)\n",
            "llama_print_timings:        eval time =    5095.23 ms /    96 runs   (   53.08 ms per token,    18.84 tokens per second)\n",
            "llama_print_timings:       total time =    5609.92 ms /   114 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Hub"
      ],
      "metadata": {
        "id": "kv4HJcEnwaDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "username = \"cosmo3769\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        ")"
      ],
      "metadata": {
        "id": "9kzWGyyGm_Ji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "e053b85180124ce9a5bac5859ce12078",
            "86f42c928f12401faddd40522373f698",
            "2edb4729e29549a2987c51368eac8dc4",
            "8098a2bdd5d44828821c0bded96cc99c",
            "b47e9b29e97c433cbf1eb949a39b3063",
            "3807a80d575e4cd4932a4adcdfd8c77d",
            "fea388b6cb534b3e9d3f8ec1964eca9f",
            "98592ed935194b5db999b25fd58fb30a",
            "6b15b2a75d6e4bf8b55bb6ece1da089d",
            "1045d0335a3c4cbc9bb28f157356aa74",
            "ba4ee6178a1043c0b8a40c283f04a322"
          ]
        },
        "outputId": "c6d2d022-b323-4fde-ff97-cbc0abce6bcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "starcoderbase-3b.Q4_K_M.gguf:   0%|          | 0.00/2.05G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e053b85180124ce9a5bac5859ce12078"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/cosmo3769/starcoderbase-3b-GGUF/commit/bbc345bd427c0af9cb593654e93951a063cd6f82', commit_message='Upload folder using huggingface_hub', commit_description='', oid='bbc345bd427c0af9cb593654e93951a063cd6f82', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phbdtDIZwUiH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}