{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Run evaluations on starcoderbase-3b non-quantized model\n\n[HuggingFace Model card for starcoderbase-3b](https://huggingface.co/bigcode/starcoderbase-3b)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/EleutherAI/lm-evaluation-harness\n%cd lm-evaluation-harness\n!pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:24:33.876523Z","iopub.execute_input":"2024-03-08T10:24:33.876802Z","iopub.status.idle":"2024-03-08T10:25:34.028420Z","shell.execute_reply.started":"2024-03-08T10:24:33.876776Z","shell.execute_reply":"2024-03-08T10:25:34.027209Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 32264, done.\u001b[K\nremote: Counting objects: 100% (752/752), done.\u001b[K\nremote: Compressing objects: 100% (468/468), done.\u001b[K\nremote: Total 32264 (delta 438), reused 529 (delta 281), pack-reused 31512\u001b[K\nReceiving objects: 100% (32264/32264), 22.77 MiB | 21.35 MiB/s, done.\nResolving deltas: 100% (22438/22438), done.\n/kaggle/working/lm-evaluation-harness\nObtaining file:///kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.27.2)\nCollecting evaluate (from lm_eval==0.4.1)\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting datasets>=2.16.0 (from lm_eval==0.4.1)\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\nCollecting jsonlines (from lm_eval==0.4.1)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.9.0)\nCollecting peft>=0.2.0 (from lm_eval==0.4.1)\n  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pybind11>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.11.1)\nCollecting pytablewriter (from lm_eval==0.4.1)\n  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\nCollecting rouge-score>=0.0.4 (from lm_eval==0.4.1)\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting sacrebleu>=1.5.0 (from lm_eval==0.4.1)\n  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (1.2.2)\nCollecting sqlitedict (from lm_eval==0.4.1)\n  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (2.1.2)\nCollecting tqdm-multiprocess (from lm_eval==0.4.1)\n  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: transformers>=4.1 in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (4.38.1)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.22.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (0.3.8)\nCollecting word2number (from lm_eval==0.4.1)\n  Downloading word2number-1.1.zip (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from lm_eval==0.4.1) (10.2.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.1) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.13.1)\nCollecting pyarrow>=12.0.0 (from datasets>=2.16.0->lm_eval==0.4.1)\n  Downloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pyarrow-hotfix (from datasets>=2.16.0->lm_eval==0.4.1)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.1) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->lm_eval==0.4.1) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.1) (1.16.0)\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.1)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.1) (5.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.1) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.1) (3.1.2)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==0.4.1) (0.15.2)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->lm_eval==0.4.1) (23.2.0)\nRequirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.1) (69.0.3)\nCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.4.1)\n  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.1)\n  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.1)\n  Downloading pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.1)\n  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.1)\n  Downloading tcolorpy-0.1.4-py3-none-any.whl.metadata (5.7 kB)\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1)\n  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.1) (4.0.3)\nCollecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.1)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.21.0->lm_eval==0.4.1) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.1) (2024.2.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2.8.2)\nRequirement already satisfied: pytz>=2018.9 in /opt/conda/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.1) (2023.3.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==0.4.1) (2.1.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.1) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==0.4.1) (1.3.0)\nDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.9.0-py3-none-any.whl (190 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\nDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\nDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\nDownloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\nDownloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\nDownloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\nDownloading typepy-1.3.2-py3-none-any.whl (31 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.1-0.editable-py3-none-any.whl size=15020 sha256=9ba41b78b22e906e31cf124fdf57ac151c92e50d51ee6ac89525be6f62af6331\n  Stored in directory: /tmp/pip-ephem-wheel-cache-24dq_nru/wheels/1b/1a/1b/44c80ddb18c9d7d3ce79a8d6d4561bddaddcbffb4cdfbf3259\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a4b4dbacea752a380d8be5fb5c34b1cdb6d493583cf08beb1fb51e5bc8b243e9\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=2d9f76ac29ca1dca859e434fc9670a5f1ec1df0ee2156bb391ca6ccd7408ecd0\n  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=f9b5fcb3e5a86a4e5b6eb8d2c67eeb013723d25c0881ee7fdd82f752a37e31dd\n  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\nSuccessfully built lm_eval rouge-score sqlitedict word2number\nInstalling collected packages: word2number, sqlitedict, tqdm-multiprocess, tcolorpy, pyarrow-hotfix, pyarrow, portalocker, pathvalidate, jsonlines, chardet, sacrebleu, rouge-score, mbstrdecoder, typepy, datasets, peft, evaluate, DataProperty, tabledata, pytablewriter, lm_eval\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed DataProperty-1.0.1 chardet-5.2.0 datasets-2.18.0 evaluate-0.4.1 jsonlines-4.0.0 lm_eval-0.4.1 mbstrdecoder-1.1.3 pathvalidate-3.2.0 peft-0.9.0 portalocker-2.8.2 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.0 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 word2number-1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade --q wandb\n!wandb login 26dc38c2c67a238da52038bccb87546d40f8d428","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:25:55.272422Z","iopub.execute_input":"2024-03-08T10:25:55.272790Z","iopub.status.idle":"2024-03-08T10:26:14.723534Z","shell.execute_reply.started":"2024-03-08T10:25:55.272756Z","shell.execute_reply":"2024-03-08T10:26:14.722624Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"!git config --global credential.helper store\n!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:26:22.205876Z","iopub.execute_input":"2024-03-08T10:26:22.206662Z","iopub.status.idle":"2024-03-08T10:26:24.932990Z","shell.execute_reply.started":"2024-03-08T10:26:22.206628Z","shell.execute_reply":"2024-03-08T10:26:24.931858Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,max_new_tokens=512,trust_remote_code=True \\\n    --tasks bigbench_code_line_description_generate_until,bigbench_code_line_description_multiple_choice,code2text_go,code2text_java,code2text_javascript,code2text_php,code2text_python,code2text_ruby,codexglue_code2text \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 50 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:58:56.129459Z","iopub.execute_input":"2024-03-08T10:58:56.130051Z","iopub.status.idle":"2024-03-08T11:00:09.059639Z","shell.execute_reply.started":"2024-03-08T10:58:56.130020Z","shell.execute_reply":"2024-03-08T11:00:09.058488Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2024-03-08 10:59:00.913707: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 10:59:00.913772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 10:59:00.915216: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_105907-rw2424oq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtoasty-sponge-15\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/rw2424oq\u001b[0m\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 162, in simple_evaluate\n    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/api/model.py\", line 132, in create_from_arg_string\n    return cls(**args, **args2)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 201, in __init__\n    self._create_model(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 524, in _create_model\n    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3375, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\nTypeError: GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument 'max_new_tokens'\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 162, in simple_evaluate\n    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/api/model.py\", line 132, in create_from_arg_string\n    return cls(**args, **args2)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 201, in __init__\n    self._create_model(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 524, in _create_model\n    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3375, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\nTypeError: GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument 'max_new_tokens'\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mtoasty-sponge-15\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/rw2424oq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_105907-rw2424oq/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## bigbench_code_line_description_generate_until","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks bigbench_code_line_description_generate_until \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:05:27.699708Z","iopub.execute_input":"2024-03-08T11:05:27.700389Z","iopub.status.idle":"2024-03-08T11:08:06.166790Z","shell.execute_reply.started":"2024-03-08T11:05:27.700351Z","shell.execute_reply":"2024-03-08T11:08:06.165620Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"2024-03-08 11:05:32.574378: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:05:32.574431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:05:32.575816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_110539-iagow50m\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-night-16\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/iagow50m\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:48<00:00, 24.07s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 22513.71it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nRunning generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:12<00:00,  1.28s/it]\nhf (pretrained=bigcode/starcoderbase-3b,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto\n|                    Tasks                    |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n|---------------------------------------------|------:|------|------|-----------|----:|---|-----:|\n|bigbench_code_line_description_generate_until|      1|none  |None  |exact_match|    0|¬±  |     0|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:              bigbench_code_line_description_generate_until/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgentle-night-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/iagow50m\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_110539-iagow50m/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## bigbench_code_line_description_multiple_choice","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks bigbench_code_line_description_multiple_choice \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:08:19.626803Z","iopub.execute_input":"2024-03-08T11:08:19.627662Z","iopub.status.idle":"2024-03-08T11:10:40.050958Z","shell.execute_reply.started":"2024-03-08T11:08:19.627623Z","shell.execute_reply":"2024-03-08T11:10:40.049914Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2024-03-08 11:08:24.363874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:08:24.363933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:08:24.365430: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_110830-hyftnxco\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msage-salad-17\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/hyftnxco\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.63s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 1957.03it/s]\nRunning loglikelihood requests:   0%|                    | 0/40 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\nDetermined largest batch size: 16\nRunning loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:34<00:00,  1.17it/s]\nhf (pretrained=bigcode/starcoderbase-3b,trust_remote_code=True), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto (16)\n|                    Tasks                     |Version|Filter|n-shot|Metric|Value|   |Stderr|\n|----------------------------------------------|------:|------|------|------|----:|---|-----:|\n|bigbench_code_line_description_multiple_choice|      0|none  |None  |acc   |  0.2|¬±  |0.1333|\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr 0.13333\n\u001b[34m\u001b[1mwandb\u001b[0m:      bigbench_code_line_description_multiple_choice/alias bigbench_code_line_d...\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msage-salad-17\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/hyftnxco\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_110830-hyftnxco/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## code2text_go","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_go \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:11:29.773001Z","iopub.execute_input":"2024-03-08T11:11:29.773734Z","iopub.status.idle":"2024-03-08T11:13:47.732425Z","shell.execute_reply.started":"2024-03-08T11:11:29.773697Z","shell.execute_reply":"2024-03-08T11:13:47.731300Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"2024-03-08 11:11:34.536786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:11:34.536852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:11:34.538944: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_111141-73oou38a\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-plasma-18\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/73oou38a\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.31s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14193.92it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 139, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 139, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmorning-plasma-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/73oou38a\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_111141-73oou38a/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## code2text_java","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_java \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:14:05.215487Z","iopub.execute_input":"2024-03-08T11:14:05.215860Z","iopub.status.idle":"2024-03-08T11:16:20.420830Z","shell.execute_reply.started":"2024-03-08T11:14:05.215830Z","shell.execute_reply":"2024-03-08T11:16:20.419896Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2024-03-08 11:14:09.978116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:14:09.978185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:14:09.979695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_111416-39uosb3b\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisunderstood-dawn-19\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/39uosb3b\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.38s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14905.13it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 311, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 311, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmisunderstood-dawn-19\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/39uosb3b\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_111416-39uosb3b/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## code2text_javascript","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_javascript \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:16:42.295151Z","iopub.execute_input":"2024-03-08T11:16:42.295570Z","iopub.status.idle":"2024-03-08T11:18:18.338521Z","shell.execute_reply.started":"2024-03-08T11:16:42.295536Z","shell.execute_reply":"2024-03-08T11:18:18.337369Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"2024-03-08 11:16:47.036961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:16:47.037018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:16:47.038512: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_111653-jrtbtaif\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-sky-20\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/jrtbtaif\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.33s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14691.08it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 286, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 286, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdivine-sky-20\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/jrtbtaif\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_111653-jrtbtaif/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## code2text_php","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_php \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:18:18.340487Z","iopub.execute_input":"2024-03-08T11:18:18.340784Z","iopub.status.idle":"2024-03-08T11:21:02.171880Z","shell.execute_reply.started":"2024-03-08T11:18:18.340757Z","shell.execute_reply":"2024-03-08T11:21:02.170690Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"2024-03-08 11:18:23.253780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:18:23.253836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:18:23.255340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_111830-odo5zfst\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mresilient-snowball-21\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/odo5zfst\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.33s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 12542.78it/s]\nRunning generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 270, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 270, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mresilient-snowball-21\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/odo5zfst\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_111830-odo5zfst/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## code2text_python","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_python \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## code2text_ruby","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks code2text_ruby \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## codexglue_code2text","metadata":{}},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=bigcode/starcoderbase-3b,trust_remote_code=True \\\n    --tasks codexglue_code2text \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{"execution":{"iopub.status.busy":"2024-03-08T11:25:54.572708Z","iopub.execute_input":"2024-03-08T11:25:54.573439Z","iopub.status.idle":"2024-03-08T11:33:19.971294Z","shell.execute_reply.started":"2024-03-08T11:25:54.573398Z","shell.execute_reply":"2024-03-08T11:33:19.970360Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"2024-03-08 11:25:59.477042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-08 11:25:59.477103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-08 11:25:59.478612: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/lm-evaluation-harness/wandb/run-20240308_112606-x7aiqs2i\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchocolate-sunset-22\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/x7aiqs2i\u001b[0m\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.29s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14473.10it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14614.30it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 16175.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 17697.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 12969.40it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 17970.45it/s]\nRunning generate_until requests:   0%|                   | 0/60 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 716, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/lm_eval\", line 8, in <module>\n    sys.exit(cli_evaluate())\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/__main__.py\", line 318, in cli_evaluate\n    results = evaluator.simple_evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n    results = evaluate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/utils.py\", line 288, in _wrapper\n    return fn(*args, **kwargs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/evaluator.py\", line 368, in evaluate\n    resps = getattr(lm, reqtype)(cloned_reqs)\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 1186, in generate_until\n    cont = self._model_generate(\n  File \"/kaggle/working/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 764, in _model_generate\n    return self.model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1466, in generate\n    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n    raise ValueError(\nValueError: Input length of input_ids is 716, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mchocolate-sunset-22\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark/runs/x7aiqs2i\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240308_112606-x7aiqs2i/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Running evaluations on starcoderbase-3b quantized model [GPTQ]\n\n[HuggingFace Model card for starcoderbase-3b-GPTQ](https://huggingface.co/cosmo3769/starcoderbase-3b-GPTQ)","metadata":{}},{"cell_type":"code","source":"!pip install -e .[gptq]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!lm_eval \\\n    --model hf \\\n    --model_args pretrained=cosmo3769/starcoderbase-3b-GPTQ,autogptq=True,gptq_use_triton=True \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size auto \\\n    --output_path output/starcoderbase-3b-GPTQ \\\n    --limit 10 \\\n    --wandb_args project=llm_eval_benchmark \\\n    --log_samples","metadata":{},"execution_count":null,"outputs":[]}]}