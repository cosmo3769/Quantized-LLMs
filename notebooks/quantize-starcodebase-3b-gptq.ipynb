{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:39:38.601256Z","iopub.execute_input":"2024-03-05T09:39:38.602039Z","iopub.status.idle":"2024-03-05T09:39:55.265145Z","shell.execute_reply.started":"2024-03-05T09:39:38.602003Z","shell.execute_reply":"2024-03-05T09:39:55.264054Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import random\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer\n\n\n# Define base model and output directory\nmodel_id = \"bigcode/starcoderbase-3b\"\nout_dir = model_id + \"-GPTQ\"","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:40:13.575297Z","iopub.execute_input":"2024-03-05T09:40:13.575997Z","iopub.status.idle":"2024-03-05T09:40:21.206865Z","shell.execute_reply.started":"2024-03-05T09:40:13.575963Z","shell.execute_reply":"2024-03-05T09:40:21.206040Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:40:28.495016Z","iopub.execute_input":"2024-03-05T09:40:28.496090Z","iopub.status.idle":"2024-03-05T09:40:28.548227Z","shell.execute_reply.started":"2024-03-05T09:40:28.496052Z","shell.execute_reply":"2024-03-05T09:40:28.546984Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:42:46.621462Z","iopub.execute_input":"2024-03-05T09:42:46.621839Z","iopub.status.idle":"2024-03-05T09:42:47.611972Z","shell.execute_reply.started":"2024-03-05T09:42:46.621811Z","shell.execute_reply":"2024-03-05T09:42:47.610678Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:43:01.540500Z","iopub.execute_input":"2024-03-05T09:43:01.541339Z","iopub.status.idle":"2024-03-05T09:43:03.242430Z","shell.execute_reply.started":"2024-03-05T09:43:01.541303Z","shell.execute_reply":"2024-03-05T09:43:03.241319Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:43:04.837463Z","iopub.execute_input":"2024-03-05T09:43:04.838743Z","iopub.status.idle":"2024-03-05T09:46:45.217128Z","shell.execute_reply.started":"2024-03-05T09:43:04.838706Z","shell.execute_reply":"2024-03-05T09:46:45.216235Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebee62d957fb43188028d4d0d174bdfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/32.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63ec28056aa4451bef13d16509d37d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecad4271cecc484a8a4a81a437398f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb9711ff8ac4a32a3dfabf6d2a9193d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/2.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088503cba73746a7bbfdbfb96f1aed9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b01ffe94ea24b73bd8f1c1a7e0ce359"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4f9c8a3a5e452fbc3e80e9ba4ca02c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18cc035315364a44b682820ecbd75017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"473589dca1ce44e8ac02751ef4c5e9e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bbb92eb1e134981bcc8671f6314e784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e48803ff89c4f64a1beb84d961ae906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/532 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd30b59ad5b4e35928958abb9c0671c"}},"metadata":{}}]},{"cell_type":"code","source":"# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:47:41.777898Z","iopub.execute_input":"2024-03-05T09:47:41.778874Z","iopub.status.idle":"2024-03-05T09:48:13.150603Z","shell.execute_reply.started":"2024-03-05T09:47:41.778830Z","shell.execute_reply":"2024-03-05T09:48:13.149532Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ea1850318f4d51b0eae7d1fff948b4"}},"metadata":{}}]},{"cell_type":"code","source":"examples = [\n    tokenizer(\n        \"def add(x, y): \\n z = x+y \\n return z\"\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:50:44.677376Z","iopub.execute_input":"2024-03-05T09:50:44.678326Z","iopub.status.idle":"2024-03-05T09:50:44.692781Z","shell.execute_reply.started":"2024-03-05T09:50:44.678287Z","shell.execute_reply":"2024-03-05T09:50:44.691931Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Quantize with GPTQ\nmodel.quantize(\n    examples,\n    batch_size=1,\n    use_triton=True,\n)\n\n# Save model and tokenizer\nmodel.save_quantized(out_dir, use_safetensors=True)\ntokenizer.save_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:51:16.848167Z","iopub.execute_input":"2024-03-05T09:51:16.848930Z","iopub.status.idle":"2024-03-05T09:59:39.712846Z","shell.execute_reply.started":"2024-03-05T09:51:16.848894Z","shell.execute_reply":"2024-03-05T09:59:39.711927Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"WARNING - triton is not installed, reset use_triton to False\nINFO - Start quantizing layer 1/36\nINFO - Quantizing attn.c_attn in layer 1/36...\nINFO - Quantizing attn.c_proj in layer 1/36...\nINFO - Quantizing mlp.c_fc in layer 1/36...\nINFO - Quantizing mlp.c_proj in layer 1/36...\nINFO - Start quantizing layer 2/36\nINFO - Quantizing attn.c_attn in layer 2/36...\nINFO - Quantizing attn.c_proj in layer 2/36...\nINFO - Quantizing mlp.c_fc in layer 2/36...\nINFO - Quantizing mlp.c_proj in layer 2/36...\nINFO - Start quantizing layer 3/36\nINFO - Quantizing attn.c_attn in layer 3/36...\nINFO - Quantizing attn.c_proj in layer 3/36...\nINFO - Quantizing mlp.c_fc in layer 3/36...\nINFO - Quantizing mlp.c_proj in layer 3/36...\nINFO - Start quantizing layer 4/36\nINFO - Quantizing attn.c_attn in layer 4/36...\nINFO - Quantizing attn.c_proj in layer 4/36...\nINFO - Quantizing mlp.c_fc in layer 4/36...\nINFO - Quantizing mlp.c_proj in layer 4/36...\nINFO - Start quantizing layer 5/36\nINFO - Quantizing attn.c_attn in layer 5/36...\nINFO - Quantizing attn.c_proj in layer 5/36...\nINFO - Quantizing mlp.c_fc in layer 5/36...\nINFO - Quantizing mlp.c_proj in layer 5/36...\nINFO - Start quantizing layer 6/36\nINFO - Quantizing attn.c_attn in layer 6/36...\nINFO - Quantizing attn.c_proj in layer 6/36...\nINFO - Quantizing mlp.c_fc in layer 6/36...\nINFO - Quantizing mlp.c_proj in layer 6/36...\nINFO - Start quantizing layer 7/36\nINFO - Quantizing attn.c_attn in layer 7/36...\nINFO - Quantizing attn.c_proj in layer 7/36...\nINFO - Quantizing mlp.c_fc in layer 7/36...\nINFO - Quantizing mlp.c_proj in layer 7/36...\nINFO - Start quantizing layer 8/36\nINFO - Quantizing attn.c_attn in layer 8/36...\nINFO - Quantizing attn.c_proj in layer 8/36...\nINFO - Quantizing mlp.c_fc in layer 8/36...\nINFO - Quantizing mlp.c_proj in layer 8/36...\nINFO - Start quantizing layer 9/36\nINFO - Quantizing attn.c_attn in layer 9/36...\nINFO - Quantizing attn.c_proj in layer 9/36...\nINFO - Quantizing mlp.c_fc in layer 9/36...\nINFO - Quantizing mlp.c_proj in layer 9/36...\nINFO - Start quantizing layer 10/36\nINFO - Quantizing attn.c_attn in layer 10/36...\nINFO - Quantizing attn.c_proj in layer 10/36...\nINFO - Quantizing mlp.c_fc in layer 10/36...\nINFO - Quantizing mlp.c_proj in layer 10/36...\nINFO - Start quantizing layer 11/36\nINFO - Quantizing attn.c_attn in layer 11/36...\nINFO - Quantizing attn.c_proj in layer 11/36...\nINFO - Quantizing mlp.c_fc in layer 11/36...\nINFO - Quantizing mlp.c_proj in layer 11/36...\nINFO - Start quantizing layer 12/36\nINFO - Quantizing attn.c_attn in layer 12/36...\nINFO - Quantizing attn.c_proj in layer 12/36...\nINFO - Quantizing mlp.c_fc in layer 12/36...\nINFO - Quantizing mlp.c_proj in layer 12/36...\nINFO - Start quantizing layer 13/36\nINFO - Quantizing attn.c_attn in layer 13/36...\nINFO - Quantizing attn.c_proj in layer 13/36...\nINFO - Quantizing mlp.c_fc in layer 13/36...\nINFO - Quantizing mlp.c_proj in layer 13/36...\nINFO - Start quantizing layer 14/36\nINFO - Quantizing attn.c_attn in layer 14/36...\nINFO - Quantizing attn.c_proj in layer 14/36...\nINFO - Quantizing mlp.c_fc in layer 14/36...\nINFO - Quantizing mlp.c_proj in layer 14/36...\nINFO - Start quantizing layer 15/36\nINFO - Quantizing attn.c_attn in layer 15/36...\nINFO - Quantizing attn.c_proj in layer 15/36...\nINFO - Quantizing mlp.c_fc in layer 15/36...\nINFO - Quantizing mlp.c_proj in layer 15/36...\nINFO - Start quantizing layer 16/36\nINFO - Quantizing attn.c_attn in layer 16/36...\nINFO - Quantizing attn.c_proj in layer 16/36...\nINFO - Quantizing mlp.c_fc in layer 16/36...\nINFO - Quantizing mlp.c_proj in layer 16/36...\nINFO - Start quantizing layer 17/36\nINFO - Quantizing attn.c_attn in layer 17/36...\nINFO - Quantizing attn.c_proj in layer 17/36...\nINFO - Quantizing mlp.c_fc in layer 17/36...\nINFO - Quantizing mlp.c_proj in layer 17/36...\nINFO - Start quantizing layer 18/36\nINFO - Quantizing attn.c_attn in layer 18/36...\nINFO - Quantizing attn.c_proj in layer 18/36...\nINFO - Quantizing mlp.c_fc in layer 18/36...\nINFO - Quantizing mlp.c_proj in layer 18/36...\nINFO - Start quantizing layer 19/36\nINFO - Quantizing attn.c_attn in layer 19/36...\nINFO - Quantizing attn.c_proj in layer 19/36...\nINFO - Quantizing mlp.c_fc in layer 19/36...\nINFO - Quantizing mlp.c_proj in layer 19/36...\nINFO - Start quantizing layer 20/36\nINFO - Quantizing attn.c_attn in layer 20/36...\nINFO - Quantizing attn.c_proj in layer 20/36...\nINFO - Quantizing mlp.c_fc in layer 20/36...\nINFO - Quantizing mlp.c_proj in layer 20/36...\nINFO - Start quantizing layer 21/36\nINFO - Quantizing attn.c_attn in layer 21/36...\nINFO - Quantizing attn.c_proj in layer 21/36...\nINFO - Quantizing mlp.c_fc in layer 21/36...\nINFO - Quantizing mlp.c_proj in layer 21/36...\nINFO - Start quantizing layer 22/36\nINFO - Quantizing attn.c_attn in layer 22/36...\nINFO - Quantizing attn.c_proj in layer 22/36...\nINFO - Quantizing mlp.c_fc in layer 22/36...\nINFO - Quantizing mlp.c_proj in layer 22/36...\nINFO - Start quantizing layer 23/36\nINFO - Quantizing attn.c_attn in layer 23/36...\nINFO - Quantizing attn.c_proj in layer 23/36...\nINFO - Quantizing mlp.c_fc in layer 23/36...\nINFO - Quantizing mlp.c_proj in layer 23/36...\nINFO - Start quantizing layer 24/36\nINFO - Quantizing attn.c_attn in layer 24/36...\nINFO - Quantizing attn.c_proj in layer 24/36...\nINFO - Quantizing mlp.c_fc in layer 24/36...\nINFO - Quantizing mlp.c_proj in layer 24/36...\nINFO - Start quantizing layer 25/36\nINFO - Quantizing attn.c_attn in layer 25/36...\nINFO - Quantizing attn.c_proj in layer 25/36...\nINFO - Quantizing mlp.c_fc in layer 25/36...\nINFO - Quantizing mlp.c_proj in layer 25/36...\nINFO - Start quantizing layer 26/36\nINFO - Quantizing attn.c_attn in layer 26/36...\nINFO - Quantizing attn.c_proj in layer 26/36...\nINFO - Quantizing mlp.c_fc in layer 26/36...\nINFO - Quantizing mlp.c_proj in layer 26/36...\nINFO - Start quantizing layer 27/36\nINFO - Quantizing attn.c_attn in layer 27/36...\nINFO - Quantizing attn.c_proj in layer 27/36...\nINFO - Quantizing mlp.c_fc in layer 27/36...\nINFO - Quantizing mlp.c_proj in layer 27/36...\nINFO - Start quantizing layer 28/36\nINFO - Quantizing attn.c_attn in layer 28/36...\nINFO - Quantizing attn.c_proj in layer 28/36...\nINFO - Quantizing mlp.c_fc in layer 28/36...\nINFO - Quantizing mlp.c_proj in layer 28/36...\nINFO - Start quantizing layer 29/36\nINFO - Quantizing attn.c_attn in layer 29/36...\nINFO - Quantizing attn.c_proj in layer 29/36...\nINFO - Quantizing mlp.c_fc in layer 29/36...\nINFO - Quantizing mlp.c_proj in layer 29/36...\nINFO - Start quantizing layer 30/36\nINFO - Quantizing attn.c_attn in layer 30/36...\nINFO - Quantizing attn.c_proj in layer 30/36...\nINFO - Quantizing mlp.c_fc in layer 30/36...\nINFO - Quantizing mlp.c_proj in layer 30/36...\nINFO - Start quantizing layer 31/36\nINFO - Quantizing attn.c_attn in layer 31/36...\nINFO - Quantizing attn.c_proj in layer 31/36...\nINFO - Quantizing mlp.c_fc in layer 31/36...\nINFO - Quantizing mlp.c_proj in layer 31/36...\nINFO - Start quantizing layer 32/36\nINFO - Quantizing attn.c_attn in layer 32/36...\nINFO - Quantizing attn.c_proj in layer 32/36...\nINFO - Quantizing mlp.c_fc in layer 32/36...\nINFO - Quantizing mlp.c_proj in layer 32/36...\nINFO - Start quantizing layer 33/36\nINFO - Quantizing attn.c_attn in layer 33/36...\nINFO - Quantizing attn.c_proj in layer 33/36...\nINFO - Quantizing mlp.c_fc in layer 33/36...\nINFO - Quantizing mlp.c_proj in layer 33/36...\nINFO - Start quantizing layer 34/36\nINFO - Quantizing attn.c_attn in layer 34/36...\nINFO - Quantizing attn.c_proj in layer 34/36...\nINFO - Quantizing mlp.c_fc in layer 34/36...\nINFO - Quantizing mlp.c_proj in layer 34/36...\nINFO - Start quantizing layer 35/36\nINFO - Quantizing attn.c_attn in layer 35/36...\nINFO - Quantizing attn.c_proj in layer 35/36...\nINFO - Quantizing mlp.c_fc in layer 35/36...\nINFO - Quantizing mlp.c_proj in layer 35/36...\nINFO - Start quantizing layer 36/36\nINFO - Quantizing attn.c_attn in layer 36/36...\nINFO - Quantizing attn.c_proj in layer 36/36...\nINFO - Quantizing mlp.c_fc in layer 36/36...\nINFO - Quantizing mlp.c_proj in layer 36/36...\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 9min 10s, sys: 40.9 s, total: 9min 51s\nWall time: 8min 22s\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('bigcode/starcoderbase-3b-GPTQ/tokenizer_config.json',\n 'bigcode/starcoderbase-3b-GPTQ/special_tokens_map.json',\n 'bigcode/starcoderbase-3b-GPTQ/vocab.json',\n 'bigcode/starcoderbase-3b-GPTQ/merges.txt',\n 'bigcode/starcoderbase-3b-GPTQ/added_tokens.json',\n 'bigcode/starcoderbase-3b-GPTQ/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Reload model and tokenizer\nmodel = AutoGPTQForCausalLM.from_quantized(\n    out_dir,\n    device=device,\n    use_triton=True,\n    use_safetensors=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:59:51.802640Z","iopub.execute_input":"2024-03-05T09:59:51.802996Z","iopub.status.idle":"2024-03-05T09:59:53.972455Z","shell.execute_reply.started":"2024-03-05T09:59:51.802971Z","shell.execute_reply":"2024-03-05T09:59:53.971311Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"WARNING - Triton is not installed, reset use_triton to False.\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n","output_type":"stream"}]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:00:14.864697Z","iopub.execute_input":"2024-03-05T10:00:14.865341Z","iopub.status.idle":"2024-03-05T10:00:14.871597Z","shell.execute_reply.started":"2024-03-05T10:00:14.865310Z","shell.execute_reply":"2024-03-05T10:00:14.870583Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'UTF-8'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline, TextGenerationPipeline\n\n# or you can also use pipeline\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"def add(x, y)\")[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:01:55.110605Z","iopub.execute_input":"2024-03-05T10:01:55.111427Z","iopub.status.idle":"2024-03-05T10:01:59.431158Z","shell.execute_reply.started":"2024-03-05T10:01:55.111390Z","shell.execute_reply":"2024-03-05T10:01:59.430069Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The model 'GPTBigCodeGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"def add(x, y) {\n    return x + y;\n}\n\nfunction add\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q huggingface_hub\nfrom huggingface_hub import create_repo, HfApi\n# from google.colab import userdata\n\nusername = \"cosmo3769\"\nMODEL_NAME = \"starcoderbase-3b\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=\"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\")\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GPTQ\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=\"/kaggle/working/bigcode/starcoderbase-3b-GPTQ\",\n    repo_id=f\"{username}/{MODEL_NAME}-GPTQ\",\n    allow_patterns=f\"*.gptq\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:11:52.924291Z","iopub.execute_input":"2024-03-05T10:11:52.925242Z","iopub.status.idle":"2024-03-05T10:12:06.765543Z","shell.execute_reply.started":"2024-03-05T10:11:52.925204Z","shell.execute_reply":"2024-03-05T10:12:06.764379Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/cosmo3769/starcoderbase-3b-GPTQ/commit/08230728044237ece77382743c1543e3a2ccf6e5', commit_message='Upload folder using huggingface_hub', commit_description='', oid='08230728044237ece77382743c1543e3a2ccf6e5', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}