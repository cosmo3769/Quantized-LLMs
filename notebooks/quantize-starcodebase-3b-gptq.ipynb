{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:39:38.601256Z","iopub.execute_input":"2024-03-05T09:39:38.602039Z","iopub.status.idle":"2024-03-05T09:39:55.265145Z","shell.execute_reply.started":"2024-03-05T09:39:38.602003Z","shell.execute_reply":"2024-03-05T09:39:55.264054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer\n\n\n# Define base model and output directory\nmodel_id = \"bigcode/starcoderbase-3b\"\nout_dir = model_id + \"-GPTQ\"","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:40:13.575297Z","iopub.execute_input":"2024-03-05T09:40:13.575997Z","iopub.status.idle":"2024-03-05T09:40:21.206865Z","shell.execute_reply.started":"2024-03-05T09:40:13.575963Z","shell.execute_reply":"2024-03-05T09:40:21.206040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:40:28.495016Z","iopub.execute_input":"2024-03-05T09:40:28.496090Z","iopub.status.idle":"2024-03-05T09:40:28.548227Z","shell.execute_reply.started":"2024-03-05T09:40:28.496052Z","shell.execute_reply":"2024-03-05T09:40:28.546984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:42:46.621462Z","iopub.execute_input":"2024-03-05T09:42:46.621839Z","iopub.status.idle":"2024-03-05T09:42:47.611972Z","shell.execute_reply.started":"2024-03-05T09:42:46.621811Z","shell.execute_reply":"2024-03-05T09:42:47.610678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token <token> --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:43:01.540500Z","iopub.execute_input":"2024-03-05T09:43:01.541339Z","iopub.status.idle":"2024-03-05T09:43:03.242430Z","shell.execute_reply.started":"2024-03-05T09:43:01.541303Z","shell.execute_reply":"2024-03-05T09:43:03.241319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:43:04.837463Z","iopub.execute_input":"2024-03-05T09:43:04.838743Z","iopub.status.idle":"2024-03-05T09:46:45.217128Z","shell.execute_reply.started":"2024-03-05T09:43:04.838706Z","shell.execute_reply":"2024-03-05T09:46:45.216235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:47:41.777898Z","iopub.execute_input":"2024-03-05T09:47:41.778874Z","iopub.status.idle":"2024-03-05T09:48:13.150603Z","shell.execute_reply.started":"2024-03-05T09:47:41.778830Z","shell.execute_reply":"2024-03-05T09:48:13.149532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = [\n    tokenizer(\n        \"def add(x, y): \\n z = x+y \\n return z\"\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:50:44.677376Z","iopub.execute_input":"2024-03-05T09:50:44.678326Z","iopub.status.idle":"2024-03-05T09:50:44.692781Z","shell.execute_reply.started":"2024-03-05T09:50:44.678287Z","shell.execute_reply":"2024-03-05T09:50:44.691931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Quantize with GPTQ\nmodel.quantize(\n    examples,\n    batch_size=1,\n    use_triton=True,\n)\n\n# Save model and tokenizer\nmodel.save_quantized(out_dir, use_safetensors=True)\ntokenizer.save_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:51:16.848167Z","iopub.execute_input":"2024-03-05T09:51:16.848930Z","iopub.status.idle":"2024-03-05T09:59:39.712846Z","shell.execute_reply.started":"2024-03-05T09:51:16.848894Z","shell.execute_reply":"2024-03-05T09:59:39.711927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload model and tokenizer\nmodel = AutoGPTQForCausalLM.from_quantized(\n    out_dir,\n    device=device,\n    use_triton=True,\n    use_safetensors=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T09:59:51.802640Z","iopub.execute_input":"2024-03-05T09:59:51.802996Z","iopub.status.idle":"2024-03-05T09:59:53.972455Z","shell.execute_reply.started":"2024-03-05T09:59:51.802971Z","shell.execute_reply":"2024-03-05T09:59:53.971311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:00:14.864697Z","iopub.execute_input":"2024-03-05T10:00:14.865341Z","iopub.status.idle":"2024-03-05T10:00:14.871597Z","shell.execute_reply.started":"2024-03-05T10:00:14.865310Z","shell.execute_reply":"2024-03-05T10:00:14.870583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, TextGenerationPipeline\n\n# or you can also use pipeline\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"def add(x, y)\")[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:01:55.110605Z","iopub.execute_input":"2024-03-05T10:01:55.111427Z","iopub.status.idle":"2024-03-05T10:01:59.431158Z","shell.execute_reply.started":"2024-03-05T10:01:55.111390Z","shell.execute_reply":"2024-03-05T10:01:59.430069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q huggingface_hub\nfrom huggingface_hub import create_repo, HfApi\n# from google.colab import userdata\n\nusername = \"cosmo3769\"\nMODEL_NAME = \"starcoderbase-3b\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=\"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\")\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GPTQ\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=\"/kaggle/working/bigcode/starcoderbase-3b-GPTQ\",\n    repo_id=f\"{username}/{MODEL_NAME}-GPTQ\",\n    allow_patterns=f\"*.gptq\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T10:11:52.924291Z","iopub.execute_input":"2024-03-05T10:11:52.925242Z","iopub.status.idle":"2024-03-05T10:12:06.765543Z","shell.execute_reply.started":"2024-03-05T10:11:52.925204Z","shell.execute_reply":"2024-03-05T10:12:06.764379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}