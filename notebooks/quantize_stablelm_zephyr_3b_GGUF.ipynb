{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6805Bb/k47R2qs/18sT9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91326d21b0b2416cbea04a67f0ba5a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c864e728b8954c7194437d011c5b6d24",
              "IPY_MODEL_7d1d579cfac945dc8f48c3293c323525",
              "IPY_MODEL_65adddf92a22420baf8864c4422d48e9"
            ],
            "layout": "IPY_MODEL_4939618384724ac58b52ed53042c7aff"
          }
        },
        "c864e728b8954c7194437d011c5b6d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d1b874f7424e1aa1abdeb217ef060c",
            "placeholder": "​",
            "style": "IPY_MODEL_6069da5f8ddc446e8fae4cab9c7ce832",
            "value": "stablelm-zephyr-3b.Q4_K_M.gguf: 100%"
          }
        },
        "7d1d579cfac945dc8f48c3293c323525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d7834f280c4147b5398fbcffc77910",
            "max": 1708595648,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92cae7e5de8d4600be3468a4a4cb6f58",
            "value": 1708595648
          }
        },
        "65adddf92a22420baf8864c4422d48e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_440556f23ea2428c84a501f0b26e33f9",
            "placeholder": "​",
            "style": "IPY_MODEL_716ef8bc704a4991a3e1ff7ecd63ed1e",
            "value": " 1.71G/1.71G [00:42&lt;00:00, 46.4MB/s]"
          }
        },
        "4939618384724ac58b52ed53042c7aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d1b874f7424e1aa1abdeb217ef060c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6069da5f8ddc446e8fae4cab9c7ce832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76d7834f280c4147b5398fbcffc77910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cae7e5de8d4600be3468a4a4cb6f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "440556f23ea2428c84a501f0b26e33f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "716ef8bc704a4991a3e1ff7ecd63ed1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmo3769/Quantized-LLMs/blob/main/notebooks/quantize_stablelm_zephyr_3b_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install llama.cpp"
      ],
      "metadata": {
        "id": "8MWEm0aE6fj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rHpVkDZQ1MdH",
        "outputId": "b48b2115-9de0-49b4-b5f3-d89e9ccb370e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19614, done.\u001b[K\n",
            "remote: Counting objects: 100% (6928/6928), done.\u001b[K\n",
            "remote: Compressing objects: 100% (538/538), done.\u001b[K\n",
            "remote: Total 19614 (delta 6693), reused 6468 (delta 6389), pack-reused 12686\u001b[K\n",
            "Receiving objects: 100% (19614/19614), 23.47 MiB | 17.82 MiB/s, done.\n",
            "Resolving deltas: 100% (13862/13862), done.\n",
            "Already up to date.\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey tests/test-c.o tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops tests/test-model-load-cancel tests/test-autorelease\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include \n",
            "I NVCCFLAGS: -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
            "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I NVCC:      Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/console.cpp -o console.o\n",
            "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda.cu -o ggml-cuda.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/main/main.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize/quantize.o -o quantize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize-stats/quantize-stats.o -o quantize-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/perplexity/perplexity.o -o perplexity -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/imatrix/imatrix.o -o imatrix -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/embedding/embedding.o -o embedding -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/vdot.o -o vdot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/q8dot.o -o q8dot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/simple/simple.o -o simple -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched/batched.o -o batched -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched-bench/batched-bench.o -o batched-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/save-load-state/save-load-state.o -o save-load-state -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/clip.cpp -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Iexamples/server examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/server/server.o examples/llava/clip.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/gguf/gguf.o -o gguf -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llama-bench/llama-bench.o -o llama-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/baby-llama/baby-llama.o -o baby-llama -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/beam-search/beam-search.o -o beam-search -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/speculative/speculative.o -o speculative -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/infill/infill.o -o infill -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/tokenize/tokenize.o -o tokenize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/parallel/parallel.o -o parallel -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/finetune/finetune.o -o finetune -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/export-lora/export-lora.o -o export-lora -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookahead/lookahead.o -o lookahead -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookup/lookup.o -o lookup -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/passkey/passkey.o -o passkey -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops~=0.7.0 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4d6adc540062404893f184f6e5303542"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "WL6JFw1SAgvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose Model"
      ],
      "metadata": {
        "id": "jelBX90p6jWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"stabilityai/stablelm-zephyr-3b\"\n",
        "QUANTIZATION_METHOD = \"q4_k_m\"\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]"
      ],
      "metadata": {
        "id": "dmNTS8PF2bze"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model"
      ],
      "metadata": {
        "id": "2DtVazNg6mOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmpfaT42hzk",
        "outputId": "6e5368be-687e-4452-c952-db76f4c97c61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'stablelm-zephyr-3b'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (203/203), done.\u001b[K\n",
            "remote: Total 206 (delta 119), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (206/206), 650.45 KiB | 3.65 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Model"
      ],
      "metadata": {
        "id": "8bkR96lx6qzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "# fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "# !python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
        "\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4DAECO42wob",
        "outputId": "3dee5290-6e5d-45a2-c958-9273e97798c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: stablelm-zephyr-3b\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "gguf: Adding 50009 merge(s).\n",
            "gguf: Setting special token type bos to 0\n",
            "gguf: Setting special token type eos to 0\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 0\n",
            "gguf: Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "Exporting model to 'stablelm-zephyr-3b/stablelm-zephyr-3b.fp16.bin'\n",
            "gguf: loading model part 'model.safetensors'\n",
            "output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "token_embd.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "output_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "Model successfully exported to 'stablelm-zephyr-3b/stablelm-zephyr-3b.fp16.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model\n",
        "qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "!./llama.cpp/quantize {fp16} {qtype} {QUANTIZATION_METHOD}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwm2U98Iipoo",
        "outputId": "0a5e559e-d68e-4c8b-84a8-9e21cb64df70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 2330 (5a51cc1b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'stablelm-zephyr-3b/stablelm-zephyr-3b.fp16.bin' to 'stablelm-zephyr-3b/stablelm-zephyr-3b.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 356 tensors from stablelm-zephyr-3b/stablelm-zephyr-3b.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
            "llama_model_loader: - kv   1:                               general.name str              = stablelm-zephyr-3b\n",
            "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
            "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
            "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:  130 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "llama_model_quantize_internal: meta size = 1789888 bytes\n",
            "[   1/ 356]                        output.weight - [ 2560, 50304,     1,     1], type =    f16, quantizing to q6_K .. size =   245.62 MiB ->   100.74 MiB\n",
            "[   2/ 356]                    token_embd.weight - [ 2560, 50304,     1,     1], type =    f16, quantizing to q4_K .. size =   245.62 MiB ->    69.08 MiB\n",
            "[   3/ 356]                 blk.0.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[   4/ 356]               blk.0.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[   5/ 356]                blk.0.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[   6/ 356]                blk.0.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[   7/ 356]                  blk.0.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[   8/ 356]                  blk.0.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[   9/ 356]                blk.0.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  10/ 356]                  blk.0.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  11/ 356]             blk.0.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  12/ 356]                  blk.0.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  13/ 356]                  blk.0.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  14/ 356]                 blk.1.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  15/ 356]               blk.1.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  16/ 356]                blk.1.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[  17/ 356]                blk.1.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  18/ 356]                  blk.1.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  19/ 356]                  blk.1.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  20/ 356]                blk.1.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  21/ 356]                  blk.1.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  22/ 356]             blk.1.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  23/ 356]                  blk.1.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  24/ 356]                  blk.1.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  25/ 356]                blk.10.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  26/ 356]              blk.10.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  27/ 356]               blk.10.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[  28/ 356]               blk.10.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  29/ 356]                 blk.10.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  30/ 356]                 blk.10.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  31/ 356]               blk.10.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  32/ 356]                 blk.10.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  33/ 356]            blk.10.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  34/ 356]                 blk.10.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  35/ 356]                 blk.10.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  36/ 356]                blk.11.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  37/ 356]              blk.11.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  38/ 356]               blk.11.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[  39/ 356]               blk.11.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  40/ 356]                 blk.11.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  41/ 356]                 blk.11.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  42/ 356]               blk.11.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  43/ 356]                 blk.11.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  44/ 356]            blk.11.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  45/ 356]                 blk.11.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  46/ 356]                 blk.11.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  47/ 356]                blk.12.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  48/ 356]              blk.12.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  49/ 356]               blk.12.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  50/ 356]               blk.12.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  51/ 356]                 blk.12.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  52/ 356]                 blk.12.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  53/ 356]               blk.12.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  54/ 356]                 blk.12.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  55/ 356]            blk.12.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  56/ 356]                 blk.12.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  57/ 356]                 blk.12.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  58/ 356]                blk.13.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  59/ 356]              blk.13.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  60/ 356]               blk.13.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  61/ 356]               blk.13.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  62/ 356]                 blk.13.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  63/ 356]                 blk.13.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  64/ 356]               blk.13.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  65/ 356]                 blk.13.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  66/ 356]            blk.13.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  67/ 356]                 blk.13.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  68/ 356]                 blk.13.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  69/ 356]                blk.14.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  70/ 356]              blk.14.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  71/ 356]               blk.14.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[  72/ 356]               blk.14.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  73/ 356]                 blk.14.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  74/ 356]                 blk.14.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  75/ 356]               blk.14.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  76/ 356]                 blk.14.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  77/ 356]            blk.14.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  78/ 356]                 blk.14.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  79/ 356]                 blk.14.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  80/ 356]                blk.15.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  81/ 356]              blk.15.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  82/ 356]               blk.15.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  83/ 356]               blk.15.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  84/ 356]                 blk.15.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  85/ 356]                 blk.15.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  86/ 356]               blk.15.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  87/ 356]                 blk.15.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  88/ 356]            blk.15.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  89/ 356]                 blk.15.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  90/ 356]                 blk.15.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  91/ 356]                blk.16.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  92/ 356]              blk.16.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  93/ 356]               blk.16.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  94/ 356]               blk.16.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  95/ 356]                 blk.16.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[  96/ 356]                 blk.16.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  97/ 356]               blk.16.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[  98/ 356]                 blk.16.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  99/ 356]            blk.16.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 100/ 356]                 blk.16.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 101/ 356]                 blk.16.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 102/ 356]                blk.17.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 103/ 356]              blk.17.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 104/ 356]               blk.17.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 105/ 356]               blk.17.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 106/ 356]                 blk.17.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 107/ 356]                 blk.17.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 108/ 356]               blk.17.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 109/ 356]                 blk.17.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 110/ 356]            blk.17.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 111/ 356]                 blk.17.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 112/ 356]                 blk.17.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 113/ 356]                blk.18.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 114/ 356]              blk.18.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 115/ 356]               blk.18.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 116/ 356]               blk.18.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 117/ 356]                 blk.18.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 118/ 356]                 blk.18.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 119/ 356]               blk.18.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 120/ 356]                 blk.18.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 121/ 356]            blk.18.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 122/ 356]                 blk.18.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 123/ 356]                 blk.18.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 124/ 356]                blk.19.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 125/ 356]              blk.19.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 126/ 356]               blk.19.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 127/ 356]               blk.19.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 128/ 356]                 blk.19.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 129/ 356]                 blk.19.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 130/ 356]               blk.19.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 131/ 356]                 blk.19.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 132/ 356]            blk.19.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 133/ 356]                 blk.19.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 134/ 356]                 blk.19.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 135/ 356]                 blk.2.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 136/ 356]               blk.2.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 137/ 356]                blk.2.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 138/ 356]                blk.2.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 139/ 356]                  blk.2.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 140/ 356]                  blk.2.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 141/ 356]                blk.2.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 142/ 356]                  blk.2.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 143/ 356]             blk.2.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 144/ 356]                  blk.2.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 145/ 356]                  blk.2.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 146/ 356]                blk.20.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 147/ 356]              blk.20.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 148/ 356]               blk.20.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 149/ 356]               blk.20.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 150/ 356]                 blk.20.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 151/ 356]                 blk.20.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 152/ 356]               blk.20.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 153/ 356]                 blk.20.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 154/ 356]            blk.20.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 155/ 356]                 blk.20.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 156/ 356]                 blk.20.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 157/ 356]                blk.21.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 158/ 356]              blk.21.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 159/ 356]               blk.21.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 160/ 356]               blk.21.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 161/ 356]                 blk.21.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 162/ 356]                 blk.21.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 163/ 356]               blk.21.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 164/ 356]                 blk.21.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 165/ 356]            blk.21.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 166/ 356]                 blk.21.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 167/ 356]                 blk.21.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 168/ 356]                blk.22.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 169/ 356]              blk.22.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 170/ 356]               blk.22.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 171/ 356]               blk.22.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 172/ 356]                 blk.22.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 173/ 356]                 blk.22.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 174/ 356]               blk.22.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 175/ 356]                 blk.22.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 176/ 356]            blk.22.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 177/ 356]                 blk.22.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 178/ 356]                 blk.22.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 179/ 356]                blk.23.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 180/ 356]              blk.23.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 181/ 356]               blk.23.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 182/ 356]               blk.23.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 183/ 356]                 blk.23.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 184/ 356]                 blk.23.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 185/ 356]               blk.23.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 186/ 356]                 blk.23.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 187/ 356]            blk.23.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 188/ 356]                 blk.23.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 189/ 356]                 blk.23.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 190/ 356]                blk.24.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 191/ 356]              blk.24.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 192/ 356]               blk.24.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 193/ 356]               blk.24.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 194/ 356]                 blk.24.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 195/ 356]                 blk.24.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 196/ 356]               blk.24.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 197/ 356]                 blk.24.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 198/ 356]            blk.24.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 199/ 356]                 blk.24.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 200/ 356]                 blk.24.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 201/ 356]                blk.25.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 202/ 356]              blk.25.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 203/ 356]               blk.25.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 204/ 356]               blk.25.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 205/ 356]                 blk.25.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 206/ 356]                 blk.25.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 207/ 356]               blk.25.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 208/ 356]                 blk.25.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 209/ 356]            blk.25.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 210/ 356]                 blk.25.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 211/ 356]                 blk.25.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 212/ 356]                blk.26.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 213/ 356]              blk.26.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 214/ 356]               blk.26.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 215/ 356]               blk.26.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 216/ 356]                 blk.26.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 217/ 356]                 blk.26.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 218/ 356]               blk.26.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 219/ 356]                 blk.26.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 220/ 356]            blk.26.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 221/ 356]                 blk.26.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 222/ 356]                 blk.26.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 223/ 356]                blk.27.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 224/ 356]              blk.27.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 225/ 356]               blk.27.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 226/ 356]               blk.27.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 227/ 356]                 blk.27.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 228/ 356]                 blk.27.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 229/ 356]               blk.27.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 230/ 356]                 blk.27.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 231/ 356]            blk.27.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 232/ 356]                 blk.27.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 233/ 356]                 blk.27.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 234/ 356]                blk.28.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 235/ 356]              blk.28.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 236/ 356]               blk.28.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 237/ 356]               blk.28.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 238/ 356]                 blk.28.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 239/ 356]                 blk.28.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 240/ 356]               blk.28.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 241/ 356]                 blk.28.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 242/ 356]            blk.28.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 243/ 356]                 blk.28.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 244/ 356]                 blk.28.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 245/ 356]                blk.29.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 246/ 356]              blk.29.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 247/ 356]               blk.29.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 248/ 356]               blk.29.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 249/ 356]                 blk.29.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 250/ 356]                 blk.29.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 251/ 356]               blk.29.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 252/ 356]                 blk.29.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 253/ 356]            blk.29.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 254/ 356]                 blk.29.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 255/ 356]                 blk.29.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 256/ 356]                 blk.3.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 257/ 356]               blk.3.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 258/ 356]                blk.3.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 259/ 356]                blk.3.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 260/ 356]                  blk.3.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 261/ 356]                  blk.3.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 262/ 356]                blk.3.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 263/ 356]                  blk.3.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 264/ 356]             blk.3.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 265/ 356]                  blk.3.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 266/ 356]                  blk.3.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 267/ 356]                blk.30.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 268/ 356]              blk.30.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 269/ 356]               blk.30.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 270/ 356]               blk.30.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 271/ 356]                 blk.30.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 272/ 356]                 blk.30.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 273/ 356]               blk.30.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 274/ 356]                 blk.30.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 275/ 356]            blk.30.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 276/ 356]                 blk.30.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 277/ 356]                 blk.30.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 278/ 356]                blk.31.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 279/ 356]              blk.31.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 280/ 356]               blk.31.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 281/ 356]               blk.31.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 282/ 356]                 blk.31.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 283/ 356]                 blk.31.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 284/ 356]               blk.31.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 285/ 356]                 blk.31.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 286/ 356]            blk.31.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 287/ 356]                 blk.31.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 288/ 356]                 blk.31.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 289/ 356]                 blk.4.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 290/ 356]               blk.4.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 291/ 356]                blk.4.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 292/ 356]                blk.4.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 293/ 356]                  blk.4.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 294/ 356]                  blk.4.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 295/ 356]                blk.4.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 296/ 356]                  blk.4.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 297/ 356]             blk.4.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 298/ 356]                  blk.4.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 299/ 356]                  blk.4.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 300/ 356]                 blk.5.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 301/ 356]               blk.5.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 302/ 356]                blk.5.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 303/ 356]                blk.5.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 304/ 356]                  blk.5.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 305/ 356]                  blk.5.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 306/ 356]                blk.5.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 307/ 356]                  blk.5.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 308/ 356]             blk.5.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 309/ 356]                  blk.5.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 310/ 356]                  blk.5.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 311/ 356]                 blk.6.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 312/ 356]               blk.6.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 313/ 356]                blk.6.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 314/ 356]                blk.6.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 315/ 356]                  blk.6.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 316/ 356]                  blk.6.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 317/ 356]                blk.6.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 318/ 356]                  blk.6.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 319/ 356]             blk.6.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 320/ 356]                  blk.6.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 321/ 356]                  blk.6.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 322/ 356]                 blk.7.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 323/ 356]               blk.7.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 324/ 356]                blk.7.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 325/ 356]                blk.7.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 326/ 356]                  blk.7.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 327/ 356]                  blk.7.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 328/ 356]                blk.7.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 329/ 356]                  blk.7.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 330/ 356]             blk.7.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 331/ 356]                  blk.7.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 332/ 356]                  blk.7.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 333/ 356]                 blk.8.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 334/ 356]               blk.8.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 335/ 356]                blk.8.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 336/ 356]                blk.8.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 337/ 356]                  blk.8.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 338/ 356]                  blk.8.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 339/ 356]                blk.8.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 340/ 356]                  blk.8.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 341/ 356]             blk.8.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 342/ 356]                  blk.8.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 343/ 356]                  blk.8.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 344/ 356]                 blk.9.attn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 345/ 356]               blk.9.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 346/ 356]                blk.9.ffn_down.weight - [ 6912,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    33.75 MiB ->    13.84 MiB\n",
            "[ 347/ 356]                blk.9.ffn_gate.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 348/ 356]                  blk.9.ffn_up.weight - [ 2560,  6912,     1,     1], type =    f16, quantizing to q4_K .. size =    33.75 MiB ->     9.49 MiB\n",
            "[ 349/ 356]                  blk.9.ffn_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 350/ 356]                blk.9.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 351/ 356]                  blk.9.attn_k.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 352/ 356]             blk.9.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 353/ 356]                  blk.9.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 354/ 356]                  blk.9.attn_v.weight - [ 2560,  2560,     1,     1], type =    f16, quantizing to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 355/ 356]                     output_norm.bias - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "[ 356/ 356]                   output_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n",
            "llama_model_quantize_internal: model size  =  5332.52 MB\n",
            "llama_model_quantize_internal: quant size  =  1627.74 MB\n",
            "\n",
            "main: quantize time = 318336.70 ms\n",
            "main:    total time = 318336.70 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run inference"
      ],
      "metadata": {
        "id": "LBlRiOh76vzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UrrFMokmpyb",
        "outputId": "d01a8178-86f2-4c0e-f45d-d71877be8871"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Write a Python function to add two intergers.\n",
            "Name of the model (options: stablelm-zephyr-3b.Q4_K_M.gguf): stablelm-zephyr-3b.Q4_K_M.gguf\n",
            "Log start\n",
            "main: build = 2330 (5a51cc1b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709541801\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from stablelm-zephyr-3b/stablelm-zephyr-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
            "llama_model_loader: - kv   1:                               general.name str              = stablelm-zephyr-3b\n",
            "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
            "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
            "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  130 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 31/50304 vs 52/50304 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = stablelm\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50304\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 20\n",
            "llm_load_print_meta: n_embd_head_k    = 80\n",
            "llm_load_print_meta: n_embd_head_v    = 80\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 6912\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 2.80 B\n",
            "llm_load_print_meta: model size       = 1.59 GiB (4.88 BPW) \n",
            "llm_load_print_meta: general.name     = stablelm-zephyr-3b\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    69.08 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1558.66 MiB\n",
            "............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   160.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     7.01 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   108.25 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33mWrite a Python function to add two intergers.\u001b[0m M1 and M2 are the inputs for this function which are supposed to be integers. The function should return an integer as output.\n",
            "def add_two_integers(m1: int, m2: int) -> int:\n",
            "    return m1 + m2\n",
            "\n",
            "# Test cases\n",
            "print(add_two_integers(3, 5)) # Expected output 7\n",
            "print(add_two_integers(-2, 3)) # Expected output 1\n",
            "print(add_two_integers(0, 0)) # Expected output 0 [end of text]\n",
            "\n",
            "llama_print_timings:        load time =    5377.84 ms\n",
            "llama_print_timings:      sample time =      90.26 ms /   120 runs   (    0.75 ms per token,  1329.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =      82.77 ms /    10 tokens (    8.28 ms per token,   120.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1471.24 ms /   119 runs   (   12.36 ms per token,    80.88 tokens per second)\n",
            "llama_print_timings:       total time =    1701.22 ms /   129 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{QUANTIZATION_METHOD.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILplZkx6mqrc",
        "outputId": "280e854d-9f59-4e54-e2a0-c3e031f41572"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Write a Python function to output the fibonnaci numbers till 100.\n",
            "Name of the model (options: stablelm-zephyr-3b.Q4_K_M.gguf): stablelm-zephyr-3b.Q4_K_M.gguf\n",
            "Log start\n",
            "main: build = 2330 (5a51cc1b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709541830\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from stablelm-zephyr-3b/stablelm-zephyr-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
            "llama_model_loader: - kv   1:                               general.name str              = stablelm-zephyr-3b\n",
            "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
            "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
            "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  130 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 31/50304 vs 52/50304 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = stablelm\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50304\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 20\n",
            "llm_load_print_meta: n_embd_head_k    = 80\n",
            "llm_load_print_meta: n_embd_head_v    = 80\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 6912\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 2.80 B\n",
            "llm_load_print_meta: model size       = 1.59 GiB (4.88 BPW) \n",
            "llm_load_print_meta: general.name     = stablelm-zephyr-3b\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    69.08 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1558.66 MiB\n",
            "............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   160.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     7.01 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   108.25 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33mWrite a Python function to output the fibonnaci numbers till 100.\u001b[0m\n",
            "\n",
            "Here is a Python function that outputs Fibonacci numbers until 100:\n",
            "\n",
            "```python\n",
            "def fibonacci_until_100():\n",
            "    a, b = 0, 1\n",
            "    while a <= 100:\n",
            "        print(a)\n",
            "        a, b = b, a + b\n",
            "\n",
            "fibonacci_until_100()\n",
            "```\n",
            "\n",
            "In this function, two variables `a` and `b` are initialized to `0` and `1`, respectively. The `while` loop continues as long as `a` is less than or equal to `100`. Inside the loop, the\n",
            "llama_print_timings:        load time =     605.50 ms\n",
            "llama_print_timings:      sample time =      96.42 ms /   128 runs   (    0.75 ms per token,  1327.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     106.96 ms /    15 tokens (    7.13 ms per token,   140.24 tokens per second)\n",
            "llama_print_timings:        eval time =    1543.38 ms /   127 runs   (   12.15 ms per token,    82.29 tokens per second)\n",
            "llama_print_timings:       total time =    1808.41 ms /   142 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Hub"
      ],
      "metadata": {
        "id": "dpkKw5ZJqdTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "username = \"cosmo3769\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        ")"
      ],
      "metadata": {
        "id": "9kzWGyyGm_Ji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "91326d21b0b2416cbea04a67f0ba5a57",
            "c864e728b8954c7194437d011c5b6d24",
            "7d1d579cfac945dc8f48c3293c323525",
            "65adddf92a22420baf8864c4422d48e9",
            "4939618384724ac58b52ed53042c7aff",
            "85d1b874f7424e1aa1abdeb217ef060c",
            "6069da5f8ddc446e8fae4cab9c7ce832",
            "76d7834f280c4147b5398fbcffc77910",
            "92cae7e5de8d4600be3468a4a4cb6f58",
            "440556f23ea2428c84a501f0b26e33f9",
            "716ef8bc704a4991a3e1ff7ecd63ed1e"
          ]
        },
        "outputId": "f3549274-408a-48f4-c44b-f372406ea401"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "stablelm-zephyr-3b.Q4_K_M.gguf:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91326d21b0b2416cbea04a67f0ba5a57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/cosmo3769/stablelm-zephyr-3b-GGUF/commit/f9e78a41a5ecc393e63bd18ca62f90f80ee49775', commit_message='Upload folder using huggingface_hub', commit_description='', oid='f9e78a41a5ecc393e63bd18ca62f90f80ee49775', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_iMOmkApFIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}