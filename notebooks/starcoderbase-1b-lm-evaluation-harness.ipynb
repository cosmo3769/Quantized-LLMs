{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/lm-evaluation-harness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///teamspace/studios/this_studio/lm-evaluation-harness\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.28.0)\n",
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.18.0)\n",
      "Requirement already satisfied: jsonlines in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.9.0)\n",
      "Requirement already satisfied: peft>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.10.0)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.12.0)\n",
      "Requirement already satisfied: pytablewriter in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (1.2.0)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (1.3.2)\n",
      "Requirement already satisfied: sqlitedict in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.1.0)\n",
      "Requirement already satisfied: torch>=1.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (2.1.2)\n",
      "Requirement already satisfied: tqdm-multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.0.11)\n",
      "Requirement already satisfied: transformers>=4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (4.39.1)\n",
      "Requirement already satisfied: zstandard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.22.0)\n",
      "Requirement already satisfied: dill in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (0.3.8)\n",
      "Requirement already satisfied: word2number in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (1.1)\n",
      "Requirement already satisfied: more-itertools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval==0.4.2) (10.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (24.0)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==0.4.2) (0.4.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (0.6)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.2) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->lm_eval==0.4.2) (3.9.3)\n",
      "Requirement already satisfied: responses<0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate->lm_eval==0.4.2) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.2) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.2) (1.16.0)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2) (2.8.2)\n",
      "Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2) (0.9.0)\n",
      "Requirement already satisfied: colorama in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2) (0.4.6)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2) (5.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (1.12)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval==0.4.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval==0.4.2) (12.4.99)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==0.4.2) (0.15.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonlines->lm_eval==0.4.2) (23.2.0)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (68.2.2)\n",
      "Requirement already satisfied: DataProperty<2,>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (1.0.1)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (1.1.3)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (3.2.0)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (1.3.3)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.2) (0.1.4)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2) (1.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2) (4.0.3)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==0.4.2) (2.1.5)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.2) (8.1.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.2) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==0.4.2) (1.3.0)\n",
      "Building wheels for collected packages: lm_eval\n",
      "  Building editable for lm_eval (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lm_eval: filename=lm_eval-0.4.2-0.editable-py3-none-any.whl size=16064 sha256=80f2bc94b3e4307b40655f0db619c593db5b3d427c3f118d253e5cbb00c8dd3f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m3e91x5w/wheels/65/52/b6/325c386c0fc39e293475a5cabcbd32adc46929292e33b194f4\n",
      "Successfully built lm_eval\n",
      "Installing collected packages: lm_eval\n",
      "  Attempting uninstall: lm_eval\n",
      "    Found existing installation: lm_eval 0.4.2\n",
      "    Uninstalling lm_eval-0.4.2:\n",
      "      Successfully uninstalled lm_eval-0.4.2\n",
      "Successfully installed lm_eval-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install lm-evaluation-harness\n",
    "# !git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "%cd lm-evaluation-harness\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python[server] in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.57)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (1.24.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (3.1.3)\n",
      "Requirement already satisfied: uvicorn>=0.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (0.29.0)\n",
      "Requirement already satisfied: fastapi>=0.100.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (0.110.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (2.2.1)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (2.0.0)\n",
      "Requirement already satisfied: starlette-context<0.4,>=0.3.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python[server]) (0.3.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (2.6.4)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (0.36.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (2.1.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic-settings>=2.0.1->llama-cpp-python[server]) (1.0.1)\n",
      "Requirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (4.3.0)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server]) (2.16.3)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# install llama-cpp-python[server]\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install 'llama-cpp-python[server]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment and install python-dotenv if not already installed\n",
    "# %pip install -q python-dotenv\n",
    "\n",
    "# import dotenv to load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure API key is set for WandB and Hub\n",
    "\n",
    "Create .env file in your root and set an API key value like:\n",
    "\n",
    "HUGGINGFACE_TOKEN=\"Enter your token\"\n",
    "\n",
    "WANDB_TOKEN=\"Enter your token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"WANDB_TOKEN\") == \"\":\n",
    "    print(f\"please get an API key from WandB at https://wandb.ai/authorize\")\n",
    "    print(f\"and then set your API key value in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate and store your credententials in .netrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --q wandb\n",
    "WANDB_TOKEN = os.environ.get(\"WANDB_TOKEN\")\n",
    "!wandb login $WANDB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"HUGGINGFACE_TOKEN\") == \"\":\n",
    "    print(f\"please get an API key from HuggingFace at https://huggingface.co/settings/tokens\")\n",
    "    print(f\"and then set your API key value in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate and store your credentials in `.git_credentials` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/zeus/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN --add-to-git-credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluations on starcoderbase-1b non-quantized model\n",
    "\n",
    "[HuggingFace Model card for starcoderbase-1b](https://huggingface.co/bigcode/starcoderbase-1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] bigbench_code_line_description_generate_until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_171442-ljtdnod4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-salad-21\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/ljtdnod4/workspace\u001b[0m\n",
      "2024-03-28:17:14:46,686 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:17:14:51,886 INFO     [__main__.py:335] Selected Tasks: ['bigbench_code_line_description_generate_until']\n",
      "2024-03-28:17:14:51,888 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:17:14:51,888 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'bigcode/starcoderbase-1b', 'trust_remote_code': True}\n",
      "2024-03-28:17:14:51,929 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
      "config.json: 100%|█████████████████████████| 1.05k/1.05k [00:00<00:00, 4.84MB/s]\n",
      "model.safetensors: 100%|███████████████████| 4.55G/4.55G [01:39<00:00, 45.9MB/s]\n",
      "generation_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 442kB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 677/677 [00:00<00:00, 3.91MB/s]\n",
      "vocab.json: 100%|████████████████████████████| 777k/777k [00:00<00:00, 32.6MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 442k/442k [00:00<00:00, 20.7MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 2.06M/2.06M [00:00<00:00, 30.8MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 532/532 [00:00<00:00, 2.64MB/s]\n",
      "2024-03-28:17:16:52,890 WARNING  [task.py:322] [Task: bigbench_code_line_description_generate_until] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:16:52,890 WARNING  [task.py:322] [Task: bigbench_code_line_description_generate_until] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:16:52,896 INFO     [task.py:395] Building contexts for bigbench_code_line_description_generate_until on rank 0...\n",
      "100%|████████████████████████████████████████| 60/60 [00:00<00:00, 77552.62it/s]\n",
      "2024-03-28:17:16:52,901 INFO     [evaluator.py:379] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/60 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 1\n",
      "Running generate_until requests: 100%|██████████| 60/60 [00:08<00:00,  6.83it/s]\n",
      "hf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|                    Tasks                    |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n",
      "|---------------------------------------------|------:|------|-----:|-----------|----:|---|-----:|\n",
      "|bigbench_code_line_description_generate_until|      1|none  |     0|exact_match|    0|±  |     0|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              bigbench_code_line_description_generate_until/alias bigbench_code_line_d...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdivine-salad-21\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/ljtdnod4/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_171442-ljtdnod4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n",
    "    --tasks bigbench_code_line_description_generate_until \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] bigbench_code_line_description_multiple_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_171915-gnmxr680\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdashing-cherry-22\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/gnmxr680/workspace\u001b[0m\n",
      "2024-03-28:17:19:19,854 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:17:19:25,121 WARNING  [__main__.py:316] File output/starcoderbase-1b/results.json already exists. Results will be overwritten.\n",
      "2024-03-28:17:19:25,121 INFO     [__main__.py:335] Selected Tasks: ['bigbench_code_line_description_multiple_choice']\n",
      "2024-03-28:17:19:25,122 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:17:19:25,122 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'bigcode/starcoderbase-1b', 'trust_remote_code': True}\n",
      "2024-03-28:17:19:25,165 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
      "2024-03-28:17:19:27,292 WARNING  [task.py:763] [Task: bigbench_code_line_description_multiple_choice] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-03-28:17:19:27,293 WARNING  [task.py:775] [Task: bigbench_code_line_description_multiple_choice] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-03-28:17:19:29,618 WARNING  [task.py:322] [Task: bigbench_code_line_description_multiple_choice] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:19:29,618 WARNING  [task.py:322] [Task: bigbench_code_line_description_multiple_choice] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:19:29,625 INFO     [task.py:395] Building contexts for bigbench_code_line_description_multiple_choice on rank 0...\n",
      "100%|█████████████████████████████████████████| 60/60 [00:00<00:00, 2429.27it/s]\n",
      "2024-03-28:17:19:29,654 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                   | 0/242 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100%|█████████| 242/242 [00:42<00:00,  5.70it/s]\n",
      "hf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n",
      "|                    Tasks                     |Version|Filter|n-shot|Metric|Value|   |Stderr|\n",
      "|----------------------------------------------|------:|------|-----:|------|----:|---|-----:|\n",
      "|bigbench_code_line_description_multiple_choice|      0|none  |     0|acc   | 0.15|±  |0.0465|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr 0.04649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      bigbench_code_line_description_multiple_choice/alias bigbench_code_line_d...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdashing-cherry-22\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/gnmxr680/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_171915-gnmxr680/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n",
    "    --tasks bigbench_code_line_description_multiple_choice \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] codexglue_code2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_172045-dlwqvg8y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmagic-plant-23\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/dlwqvg8y/workspace\u001b[0m\n",
      "2024-03-28:17:20:49,859 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:17:20:55,013 WARNING  [__main__.py:266]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-03-28:17:20:55,014 WARNING  [__main__.py:316] File output/starcoderbase-1b/results.json already exists. Results will be overwritten.\n",
      "2024-03-28:17:20:55,014 INFO     [__main__.py:335] Selected Tasks: ['codexglue_code2text']\n",
      "2024-03-28:17:20:55,015 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:17:20:55,015 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'bigcode/starcoderbase-1b', 'trust_remote_code': True}\n",
      "2024-03-28:17:20:55,062 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
      "2024-03-28:17:23:57,653 INFO     [task.py:395] Building contexts for code2text_javascript on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 10866.07it/s]\n",
      "2024-03-28:17:23:57,655 INFO     [task.py:395] Building contexts for code2text_python on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 13508.23it/s]\n",
      "2024-03-28:17:23:57,656 INFO     [task.py:395] Building contexts for code2text_go on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 15563.28it/s]\n",
      "2024-03-28:17:23:57,658 INFO     [task.py:395] Building contexts for code2text_php on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 13107.20it/s]\n",
      "2024-03-28:17:23:57,660 INFO     [task.py:395] Building contexts for code2text_ruby on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 13797.05it/s]\n",
      "2024-03-28:17:23:57,662 INFO     [task.py:395] Building contexts for code2text_java on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 11229.73it/s]\n",
      "2024-03-28:17:23:57,664 INFO     [evaluator.py:379] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/12 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 1\n",
      "Running generate_until requests: 100%|██████████| 12/12 [01:44<00:00,  8.71s/it]\n",
      "2024-03-28:17:25:46,801 INFO     [__main__.py:383] Logging to Weights and Biases failed due to 'def smoothed_bleu_4(references, predictions, **kwargs):\\n    predictionMap = {}\\n    goldMap = {}\\n\\n    for rid, pred in enumerate(predictions):\\n        predictionMap[rid] = [splitPuncts(pred.strip().lower())]\\n\\n    for rid, row in enumerate(references):\\n        goldMap[rid] = [splitPuncts(row.strip().lower())]\\n\\n    return bleuFromMaps(goldMap, predictionMap)[0]\\n'\n",
      "hf (pretrained=bigcode/starcoderbase-1b,trust_remote_code=True), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n",
      "|         Tasks         |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n",
      "|-----------------------|-------|------|-----:|---------------|-----:|---|-----:|\n",
      "|codexglue_code2text    |N/A    |none  |     0|smoothed_bleu_4|0.8767|±  |0.0592|\n",
      "| - code2text_go        |      1|none  |     0|smoothed_bleu_4|1.0054|±  |0.0983|\n",
      "| - code2text_java      |      1|none  |     0|smoothed_bleu_4|1.2158|±  |0.1657|\n",
      "| - code2text_javascript|      1|none  |     0|smoothed_bleu_4|0.8560|±  |0.0429|\n",
      "| - code2text_php       |      1|none  |     0|smoothed_bleu_4|0.9879|±  |0.0887|\n",
      "| - code2text_python    |      1|none  |     0|smoothed_bleu_4|1.1950|±  |0.2819|\n",
      "| - code2text_ruby      |      3|none  |     0|smoothed_bleu_4|0.0000|±  |0.0000|\n",
      "\n",
      "|      Groups       |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n",
      "|-------------------|-------|------|-----:|---------------|-----:|---|-----:|\n",
      "|codexglue_code2text|N/A    |none  |     0|smoothed_bleu_4|0.8767|±  |0.0592|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          code2text_go/alias  - code2text_go\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 1.00535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr 0.0983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_java/alias  - code2text_java\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 1.21576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr 0.16566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias  - code2text_javascr...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 0.85602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 0.04288\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         code2text_php/alias  - code2text_php\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 0.9879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr 0.08867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      code2text_python/alias  - code2text_python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 1.19499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr 0.28195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_ruby/alias  - code2text_ruby\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   codexglue_code2text/alias codexglue_code2text\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 0.87667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr 0.05923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmagic-plant-23\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/dlwqvg8y/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_172045-dlwqvg8y/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bigcode/starcoderbase-1b,trust_remote_code=True \\\n",
    "    --tasks codexglue_code2text \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b \\\n",
    "    --limit 2 \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluations on starcoderbase-1b quantized model [GGUF]\n",
    "\n",
    "[HuggingFace Model card for starcoderbase-1b-GGUF](https://huggingface.co/cosmo3769/starcoderbase-1b-GGUF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running evaluation on quantized model [GGUF], run this in terminal to host your quantized model locally using llama-cpp-python server.\n",
    "\n",
    "`python3 -m llama_cpp.server --model starcoderbase-1b/starcoderbase-1b.Q4_K_M.gguf --n_gpu_layers 10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] bigbench_code_line_description_generate_until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_172740-ybiqwns4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-dew-24\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/ybiqwns4/workspace\u001b[0m\n",
      "2024-03-28:17:27:44,954 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:17:27:50,205 INFO     [__main__.py:335] Selected Tasks: ['bigbench_code_line_description_generate_until']\n",
      "2024-03-28:17:27:50,206 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:17:27:50,206 INFO     [evaluator.py:177] Initializing gguf model, with arguments: {'base_url': 'http://localhost:8000'}\n",
      "2024-03-28:17:27:53,187 WARNING  [task.py:322] [Task: bigbench_code_line_description_generate_until] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:27:53,187 WARNING  [task.py:322] [Task: bigbench_code_line_description_generate_until] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:27:53,193 INFO     [task.py:395] Building contexts for bigbench_code_line_description_generate_until on rank 0...\n",
      "100%|████████████████████████████████████████| 60/60 [00:00<00:00, 84676.39it/s]\n",
      "2024-03-28:17:27:53,198 INFO     [evaluator.py:379] Running generate_until requests\n",
      "100%|███████████████████████████████████████████| 60/60 [03:26<00:00,  3.45s/it]\n",
      "gguf (base_url=http://localhost:8000), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|                    Tasks                    |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n",
      "|---------------------------------------------|------:|------|-----:|-----------|----:|---|-----:|\n",
      "|bigbench_code_line_description_generate_until|      1|none  |     0|exact_match|    0|±  |     0|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              bigbench_code_line_description_generate_until/alias bigbench_code_line_d...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_generate_until/exact_match 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_generate_until/exact_match_stderr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcelestial-dew-24\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/ybiqwns4/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_172740-ybiqwns4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model gguf \\\n",
    "    --model_args base_url=http://localhost:8000 \\\n",
    "    --tasks bigbench_code_line_description_generate_until \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b-GGUF \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] bigbench_code_line_description_multiple_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_173151-1pi31zqj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-shadow-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/1pi31zqj/workspace\u001b[0m\n",
      "2024-03-28:17:31:55,995 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:17:32:01,228 WARNING  [__main__.py:316] File output/starcoderbase-1b-GGUF/results.json already exists. Results will be overwritten.\n",
      "2024-03-28:17:32:01,228 INFO     [__main__.py:335] Selected Tasks: ['bigbench_code_line_description_multiple_choice']\n",
      "2024-03-28:17:32:01,229 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:17:32:01,229 INFO     [evaluator.py:177] Initializing gguf model, with arguments: {'base_url': 'http://localhost:8000'}\n",
      "2024-03-28:17:32:01,231 WARNING  [task.py:763] [Task: bigbench_code_line_description_multiple_choice] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-03-28:17:32:01,231 WARNING  [task.py:775] [Task: bigbench_code_line_description_multiple_choice] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-03-28:17:32:03,331 WARNING  [task.py:322] [Task: bigbench_code_line_description_multiple_choice] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:32:03,331 WARNING  [task.py:322] [Task: bigbench_code_line_description_multiple_choice] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-03-28:17:32:03,338 INFO     [task.py:395] Building contexts for bigbench_code_line_description_multiple_choice on rank 0...\n",
      "100%|█████████████████████████████████████████| 60/60 [00:00<00:00, 2425.41it/s]\n",
      "2024-03-28:17:32:03,367 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "100%|█████████████████████████████████████████| 242/242 [26:26<00:00,  6.56s/it]\n",
      "gguf (base_url=http://localhost:8000), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|                    Tasks                     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|----------------------------------------------|------:|------|-----:|------|-----:|---|-----:|\n",
      "|bigbench_code_line_description_multiple_choice|      0|none  |     0|acc   |0.1667|±  |0.0485|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        bigbench_code_line_description_multiple_choice/acc 0.16667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: bigbench_code_line_description_multiple_choice/acc_stderr 0.04852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      bigbench_code_line_description_multiple_choice/alias bigbench_code_line_d...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcerulean-shadow-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/1pi31zqj/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_173151-1pi31zqj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model gguf \\\n",
    "    --model_args base_url=http://localhost:8000 \\\n",
    "    --tasks bigbench_code_line_description_multiple_choice \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b-GGUF \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Benchmark] codexglue_code2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcosmo3769\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/lm-evaluation-harness/wandb/run-20240328_182426-i3lgb5r4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpretty-wind-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/i3lgb5r4/workspace\u001b[0m\n",
      "2024-03-28:18:24:32,271 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-28:18:26:05,960 WARNING  [__main__.py:266]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-03-28:18:26:05,962 WARNING  [__main__.py:316] File output/starcoderbase-1b-GGUF/results.json already exists. Results will be overwritten.\n",
      "2024-03-28:18:26:05,962 INFO     [__main__.py:335] Selected Tasks: ['codexglue_code2text']\n",
      "2024-03-28:18:26:05,987 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-03-28:18:26:05,987 INFO     [evaluator.py:177] Initializing gguf model, with arguments: {'base_url': 'http://localhost:8000'}\n",
      "2024-03-28:18:32:43,700 INFO     [task.py:395] Building contexts for code2text_ruby on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 11096.04it/s]\n",
      "2024-03-28:18:32:43,702 INFO     [task.py:395] Building contexts for code2text_python on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 15534.46it/s]\n",
      "2024-03-28:18:32:43,703 INFO     [task.py:395] Building contexts for code2text_php on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 14742.72it/s]\n",
      "2024-03-28:18:32:43,704 INFO     [task.py:395] Building contexts for code2text_javascript on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 15477.14it/s]\n",
      "2024-03-28:18:32:43,705 INFO     [task.py:395] Building contexts for code2text_java on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 15738.48it/s]\n",
      "2024-03-28:18:32:43,706 INFO     [task.py:395] Building contexts for code2text_go on rank 0...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 14438.22it/s]\n",
      "2024-03-28:18:32:43,708 INFO     [evaluator.py:379] Running generate_until requests\n",
      "100%|███████████████████████████████████████████| 12/12 [01:01<00:00,  5.16s/it]\n",
      "2024-03-28:18:33:50,777 INFO     [__main__.py:383] Logging to Weights and Biases failed due to 'def smoothed_bleu_4(references, predictions, **kwargs):\\n    predictionMap = {}\\n    goldMap = {}\\n\\n    for rid, pred in enumerate(predictions):\\n        predictionMap[rid] = [splitPuncts(pred.strip().lower())]\\n\\n    for rid, row in enumerate(references):\\n        goldMap[rid] = [splitPuncts(row.strip().lower())]\\n\\n    return bleuFromMaps(goldMap, predictionMap)[0]\\n'\n",
      "gguf (base_url=http://localhost:8000), gen_kwargs: (None), limit: 2.0, num_fewshot: None, batch_size: auto\n",
      "|         Tasks         |Version|Filter|n-shot|    Metric     | Value |   |Stderr |\n",
      "|-----------------------|-------|------|-----:|---------------|------:|---|------:|\n",
      "|codexglue_code2text    |N/A    |none  |     0|smoothed_bleu_4| 7.0254|±  | 1.8789|\n",
      "| - code2text_go        |      1|none  |     0|smoothed_bleu_4| 3.7812|±  | 3.7812|\n",
      "| - code2text_java      |      1|none  |     0|smoothed_bleu_4| 7.5486|±  | 1.3651|\n",
      "| - code2text_javascript|      1|none  |     0|smoothed_bleu_4| 3.0810|±  | 3.0810|\n",
      "| - code2text_php       |      1|none  |     0|smoothed_bleu_4|17.0558|±  |10.0261|\n",
      "| - code2text_python    |      1|none  |     0|smoothed_bleu_4|10.6860|±  | 0.9553|\n",
      "| - code2text_ruby      |      3|none  |     0|smoothed_bleu_4| 0.0000|±  | 0.0000|\n",
      "\n",
      "|      Groups       |Version|Filter|n-shot|    Metric     |Value |   |Stderr|\n",
      "|-------------------|-------|------|-----:|---------------|-----:|---|-----:|\n",
      "|codexglue_code2text|N/A    |none  |     0|smoothed_bleu_4|7.0254|±  |1.8789|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          code2text_go/alias  - code2text_go\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                code2text_go/smoothed_bleu_4 3.78116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         code2text_go/smoothed_bleu_4_stderr 3.78116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_java/alias  - code2text_java\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_java/smoothed_bleu_4 7.54864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_java/smoothed_bleu_4_stderr 1.36512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  code2text_javascript/alias  - code2text_javascr...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_javascript/smoothed_bleu_4 3.08101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: code2text_javascript/smoothed_bleu_4_stderr 3.08101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         code2text_php/alias  - code2text_php\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               code2text_php/smoothed_bleu_4 17.05583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        code2text_php/smoothed_bleu_4_stderr 10.02613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      code2text_python/alias  - code2text_python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            code2text_python/smoothed_bleu_4 10.68597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     code2text_python/smoothed_bleu_4_stderr 0.95531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        code2text_ruby/alias  - code2text_ruby\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              code2text_ruby/smoothed_bleu_4 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       code2text_ruby/smoothed_bleu_4_stderr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   codexglue_code2text/alias codexglue_code2text\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         codexglue_code2text/smoothed_bleu_4 7.02543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  codexglue_code2text/smoothed_bleu_4_stderr 1.8789\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpretty-wind-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cosmo3769/llm_eval_benchmark_lightning_ai/runs/i3lgb5r4/workspace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240328_182426-i3lgb5r4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model gguf \\\n",
    "    --model_args base_url=http://localhost:8000 \\\n",
    "    --tasks codexglue_code2text \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path output/starcoderbase-1b-GGUF \\\n",
    "    --limit 2 \\\n",
    "    --wandb_args project=llm_eval_benchmark_lightning_ai \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
